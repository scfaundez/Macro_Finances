{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nowcasting.ipynb",
      "provenance": [],
      "mount_file_id": "1ZpD1qi9O-L1Ffq4S3y8HO9EujGtM-rsH",
      "authorship_tag": "ABX9TyNRen85dDLtVWK9zSKBPv9n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scfaundez/Macro_Finances/blob/main/nowcasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isHZqi6o4fnC",
        "outputId": "f4def564-59e1-4c5e-b368-d9eac3a71163"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb-IyaV59QQe"
      },
      "source": [
        "#Load data (load_data.py)\n",
        "#-------------------------------------------------Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "#-------------------------------------------------Read data functions\n",
        "def load_data(datafile,Spec,sample = None):\n",
        "\n",
        "    # loadData Load vintage of data from file and format as structure\n",
        "    #\n",
        "    # Description:\n",
        "    #\n",
        "    #   Load data from file\n",
        "    #\n",
        "    # Input Arguments:\n",
        "    #\n",
        "    #   datafile - filename of Microsoft Excel workbook file\n",
        "    #\n",
        "    # Output Arguments:\n",
        "    #\n",
        "    # Data - structure with the following fields:\n",
        "    #   .    X : T x N numeric array, transformed dataset\n",
        "    #   . Time : T x 1 numeric array, date number with observation dates\n",
        "    #   .    Z : T x N numeric array, raw (untransformed) dataset\n",
        "\n",
        "    \"\"\"\n",
        "    Python Version Notes:\n",
        "        The original matlab function can load raw data from MATLAB formatted binary (.mat) file.\n",
        "        However, in the Python version I have removed this feature.\n",
        "\n",
        "        In transformData(), the formula_dict dictionary contains functions that transform the data. I could have done\n",
        "        it in a similar fashion as the Matlab code, however I find it easier to read the code when going through the if and elif statements\n",
        "\n",
        "        When converting dates to ordinal format, we need to add 366 days to match with Matlab date numeric values\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.splitext(datafile)[1] in [\".xlsx\",\".xls\"]:\n",
        "        ValueError(\"File is not an EXCEL FILE\")\n",
        "\n",
        "    Z,Time,Mnem = readData(datafile)\n",
        "    #    Z : raw (untransformed) observed data\n",
        "    # Time : observation periods for the time series data\n",
        "    # Mnem : series ID for each variable\n",
        "\n",
        "    # Sort data based on model specification\n",
        "    Z,_ = sortData(Z.copy(),Mnem.copy(),Spec)\n",
        "\n",
        "    # since now Mnem == Spec.SeriesID\n",
        "    del Mnem\n",
        "\n",
        "    # Transform data based on model specification\n",
        "    X,Time,Z = transformData(Z.copy(),Time.copy(),Spec)\n",
        "\n",
        "    # Drop data not in estimation sample\n",
        "    if sample != None:\n",
        "        X,Time,Z = dropData(X.copy(),Time.copy(),Z.copy(),sample)\n",
        "\n",
        "    return X,Time,Z\n",
        "\n",
        "\n",
        "def readData(datafile):\n",
        "\n",
        "    # readData Read data from Microsoft Excel workbook file\n",
        "\n",
        "    dat  = pd.read_excel(datafile)\n",
        "    Mnem = np.array([i for i in list(dat.columns) if i != \"Date\"])\n",
        "    Z    = dat[Mnem].to_numpy(copy=True)\n",
        "    Time = dat.Date.apply(lambda x: x.toordinal()+366).to_numpy(copy= True)\n",
        "\n",
        "    return Z,Time,Mnem\n",
        "\n",
        "def sortData(Z,Mnem,Spec):\n",
        "\n",
        "    # sortData Sort series by order of model specification\n",
        "\n",
        "    # Drop series not in Spec\n",
        "    inSpec = np.in1d(Mnem,Spec.SeriesID)\n",
        "    Mnem   = Mnem[inSpec]\n",
        "    Z      = Z[:,inSpec]\n",
        "\n",
        "    # Sort series by ordering of Spec\n",
        "    permutation = np.array([np.where(Mnem == i)[0][0] for i in Spec.SeriesID])\n",
        "    Mnem        = Mnem[permutation]\n",
        "    Z           = Z[:,permutation]\n",
        "\n",
        "    return Z, Mnem\n",
        "\n",
        "def transformData(Z,Time,Spec):\n",
        "\n",
        "    # transformData Transforms each data series based on Spec.Transformation\n",
        "    #\n",
        "    # Input Arguments:\n",
        "    #\n",
        "    #      Z : T x N numeric array, raw (untransformed) observed data\n",
        "    #   Spec : structure          , model specification\n",
        "    #\n",
        "    # Output Arguments:\n",
        "    #\n",
        "    #      X : T x N numeric array, transformed data (stationary to enter DFM)\n",
        "\n",
        "    \"\"\"\n",
        "    Transformation notes:\n",
        "        'lin' = Levels (No Transformation)\n",
        "        'chg' = Change (Difference)\n",
        "        'ch1' = Year over Year Change (Difference)\n",
        "        'pch' = Percent Change\n",
        "        'pc1' = Year over Year Percent Change\n",
        "        'pca' = Percent Change (Annual Rate)\n",
        "        'log' = Natural Log\n",
        "    \"\"\"\n",
        "\n",
        "    T,N          = Z.shape\n",
        "    X            = np.empty((T, N))\n",
        "    X[:]         = np.nan\n",
        "    Freq_dict    = {\"m\":1,\"q\":3}\n",
        "    formula_dict = {\"lin\":lambda x:x*2,\n",
        "                    \"chg\":lambda x:np.append(np.nan,x[t1+step::step] - x[t1:-1-t1:step]),\n",
        "                    \"ch1\":lambda x:x[12+t1::step] - x[t1:-12:step],\n",
        "                    \"pch\":lambda x:(np.append(np.nan,x[t1+step::step]/x[t1:-1-t1:step]) - 1)*100,\n",
        "                    \"pc1\":lambda x:((x[12+t1::step]/x[t1:-12:step])-1)*100,\n",
        "                    \"pca\":lambda x:(np.append(np.nan,x[t1+step::step]/x[t1:-step:step])**(1/n) - 1)*100,\n",
        "                    \"log\":lambda x:np.log(x)\n",
        "    }\n",
        "\n",
        "    for i in range(N):\n",
        "        formula = Spec.Transformation[i]\n",
        "        freq    = Spec.Frequency[i]\n",
        "        step    = Freq_dict[freq] # time step for different frequencies based on monthly time\n",
        "        t1      = step -1         # assume monthly observations start at beginning of quarter (subtracted 1 for indexing)\n",
        "        n       = step/12         # number of years, needed to compute annual % changes\n",
        "        series  = Spec.SeriesName[i]\n",
        "\n",
        "        if formula == 'lin':\n",
        "            X[:,i] = Z[:,i].copy()\n",
        "        elif formula == 'chg':\n",
        "            X[t1::step,i] = formula_dict['chg'](Z[:,i].copy())\n",
        "        elif formula == 'ch1':\n",
        "            X[12+t1::step, i] = formula_dict['ch1'](Z[:, i].copy())\n",
        "        elif formula == 'pch':\n",
        "            X[t1::step, i] = formula_dict['pch'](Z[:, i].copy())\n",
        "        elif formula == 'pc1':\n",
        "            X[12+t1::step, i] = formula_dict['pc1'](Z[:, i].copy())\n",
        "        elif formula == 'pca':\n",
        "            X[t1::step, i] = formula_dict['pca'](Z[:, i].copy())\n",
        "        elif formula == 'log':\n",
        "            X[:, i] = formula_dict['log'](Z[:, i].copy())\n",
        "        else:\n",
        "            ValueError(\"{}: Transformation is unknown\".format(formula))\n",
        "\n",
        "    # Drop first quarter of observations\n",
        "    # since transformations cause missing values\n",
        "\n",
        "    return X[3:,:],Time[3:],Z[3:,:]\n",
        "\n",
        "\n",
        "def dropData(X,Time,Z,sample):\n",
        "\n",
        "    # dropData Remove data not in estimation sample\n",
        "\n",
        "    filter_index = Time >= sample\n",
        "    X            = X[filter_index,:].copy()\n",
        "    Time         = Time[filter_index].copy()\n",
        "    Z            = Z[filter_index, :].copy()\n",
        "\n",
        "    return X,Time,Z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RkbO9ru9-un"
      },
      "source": [
        "# Load model specification for a dynamic factor model (load_spec.py)\n",
        "#-------------------------------------------------Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "\n",
        "#-------------------------------------------------load_spec class\n",
        "class load_spec():\n",
        "\n",
        "    # loadSpec Load model specification for a dynamic factor model (DFM)\n",
        "    #\n",
        "    # Description:\n",
        "    #\n",
        "    #   Load model specification  'Spec' from a Microsoft Excel workbook file\n",
        "    #   given by 'filename'.\n",
        "    #\n",
        "    # Input Arguments:\n",
        "    #\n",
        "    #   filename -\n",
        "    #\n",
        "    # Output Arguments:\n",
        "    #\n",
        "    # spec - 1 x 1 structure with the following fields:\n",
        "    #     . series_id\n",
        "    #     . name\n",
        "    #     . frequency\n",
        "    #     . units\n",
        "    #     . transformation\n",
        "    #     . category\n",
        "    #     . blocks\n",
        "    #     . BlockNames\n",
        "\n",
        "    \"\"\"\n",
        "    Python Version Notes:\n",
        "\n",
        "    spec is a dictionary containing the fields:\n",
        "        . series_id\n",
        "        . name\n",
        "        . frequency\n",
        "        . units\n",
        "        . transformation\n",
        "        . category\n",
        "        . blocks\n",
        "        . BlockNames\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self,filename):\n",
        "\n",
        "        # Find and drop series from Spec that are not in Model\n",
        "        raw         = pd.read_excel(filename)\n",
        "        raw.columns = [i.replace(\" \",\"\") for i in  raw.columns]\n",
        "        raw         = raw[raw[\"Model\"] == 1].reset_index(drop = True)\n",
        "\n",
        "        # Sort all fields of 'Spec' in order of decreasing frequency\n",
        "        frequency    = ['d','w','m','q','sa','a']\n",
        "        permutations = []\n",
        "        for freq in frequency:\n",
        "            permutations+= list(raw[raw.Frequency == freq].index)\n",
        "        raw = raw.loc[permutations,:]\n",
        "\n",
        "        # Parse fields given by column names in Excel worksheet\n",
        "        fldnms = ['SeriesID','SeriesName','Frequency','Units','Transformation','Category']\n",
        "        for field in fldnms:\n",
        "            if field in raw.columns:\n",
        "                setattr(self,field,raw[field].to_numpy(copy=True))\n",
        "            else:\n",
        "                raise ValueError(\"{} raise ValueError(column missing from model specification.\".format(field))\n",
        "\n",
        "        # Parse blocks\n",
        "        jColBlock             = list(raw.columns[raw.columns.str.contains(\"Block\", case = False)])\n",
        "        Blocks                = raw[jColBlock].copy()\n",
        "        Blocks[Blocks.isna()] = 0\n",
        "\n",
        "        if not (Blocks.iloc[:,0] == 1).all():\n",
        "            raise ValueError(\"All variables must load on global block.\")\n",
        "        else:\n",
        "            self.Blocks = Blocks.to_numpy(copy=True)\n",
        "        self.BlockNames = [re.sub(\"Block[0-9]+-\",\"\",i) for i in jColBlock]\n",
        "\n",
        "        # Transformations\n",
        "        transformation = {'lin':'Levels (No Transformation)',\n",
        "                          'chg':'Change (Difference)',\n",
        "                          'ch1':'Year over Year Change (Difference)',\n",
        "                          'pch':'Percent Change',\n",
        "                          'pc1':'Year over Year Percent Change',\n",
        "                          'pca':'Percent Change (Annual Rate)',\n",
        "                          'cch':'Continuously Compounded Rate of Change',\n",
        "                          'cca':'Continuously Compounded Annual Rate of Change',\n",
        "                          'log':'Natural Log'}\n",
        "\n",
        "        self.UnitsTransformed = np.array([transformation[i] for i in self.Transformation])\n",
        "\n",
        "        # Summarize model specification\n",
        "        print('\\n Table 1: Model specification \\n')\n",
        "        print(pd.DataFrame({\"SeriesID\"         :self.SeriesID,\n",
        "                            \"SeriesName\"       :self.SeriesName,\n",
        "                            \"Units\"            :self.Units,\n",
        "                            \"UnitsTransformed\" :self.UnitsTransformed}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yc50k_B-Kzr"
      },
      "source": [
        "#-------------------------------------------------Libraries\n",
        "import numpy as np\n",
        "from scipy.signal import lfilter\n",
        "from scipy.interpolate import CubicSpline\n",
        "\n",
        "\n",
        "#-------------------------------------------------remNaNs_spline\n",
        "def remNaNs_spline(X,options):\n",
        "    ###  Replication files for:\n",
        "    ###  \"\"Nowcasting\", 2010, (by Marta Banbura, Domenico Giannone and Lucrezia Reichlin),\n",
        "    ### in Michael P. Clements and David F. Hendry, editors, Oxford Handbook on Economic Forecasting.\n",
        "    ###\n",
        "    ### The software can be freely used in applications.\n",
        "    ### Users are kindly requested to add acknowledgements to published work and\n",
        "    ### to cite the above reference in any resulting publications\n",
        "    #\n",
        "    #Description:\n",
        "    #\n",
        "    #remNaNs    Treats NaNs in dataset for use in DFM.\n",
        "    #\n",
        "    #  Syntax:\n",
        "    #    [X,indNaN] = remNaNs(X,options)\n",
        "    #\n",
        "    #  Description:\n",
        "    #    remNaNs() processes NaNs in a data matrix X according to 5 cases (see\n",
        "    #    below for details). These are useful for running functions in the\n",
        "    #    'DFM.m' file that do not take missing value inputs.\n",
        "    #\n",
        "    #  Input parameters:\n",
        "    #    X (T x n): Input data where T gives time and n gives the series.\n",
        "    #    options: A structure with two elements:\n",
        "    #      options.method (numeric):\n",
        "    #      - 1: Replaces all missing values using filter().\n",
        "    #      - 2: Replaces missing values after removing trailing and leading\n",
        "    #           zeros (a row is 'missing' if >80# is NaN)\n",
        "    #      - 3: Only removes rows with leading and closing zeros\n",
        "    #      - 4: Replaces missing values after removing trailing and leading\n",
        "    #           zeros (a row is 'missing' if all are NaN)\n",
        "    #      - 5: Replaces missing values with spline() then runs filter().\n",
        "    #\n",
        "    #      options.k (numeric): remNaNs() relies on MATLAB's filter function\n",
        "    #      for the 1-D filter. k controls the rational transfer function\n",
        "    #      argument's numerator (the denominator is set to 1). More\n",
        "    #      specifically, the numerator takes the form 'ones(2*k+1,1)/(2*k+1)'\n",
        "    #      For additional help, see MATLAB's documentation for filter().\n",
        "    #\n",
        "    #  Output parameters:\n",
        "    #    X: Outputted data.\n",
        "    #    indNaN: A matrix indicating the location for missing values (1 for NaN).\n",
        "\n",
        "    T,N    = X.shape\n",
        "    k      = options[\"k\"]\n",
        "    indNaN = np.isnan(X)\n",
        "\n",
        "    if options[\"method\"] == 1:  # replace all the missing values\n",
        "        for i in range(N): # Loop through columns\n",
        "            x              = X[:,i].copy()\n",
        "            x[indNaN[:,i]] = np.nanmedian(x)\n",
        "            x_MA           = lfilter(np.ones((2*k+1))/(2*k+1),1,np.append(np.append(x[0]*np.ones((k,1)),x),x[-1]*np.ones((k,1))))\n",
        "            x_MA           = x_MA[(2*k+1) -1:] # Match dimensions\n",
        "            # replace all the missing values\n",
        "            x[indNaN[:,i]] = x_MA[indNaN[:,i]]\n",
        "            X[:,i]         = x # Replace vector\n",
        "\n",
        "    elif options[\"method\"] == 2: # replace missing values after removing leading and closing zeros\n",
        "        # Returns row sum for NaN values. Marks true for rows with more than 80% NaN\n",
        "        rem1    = np.nansum(indNaN, axis =1) > (N * 0.8)\n",
        "        nanLead = np.cumsum(rem1) == np.arange(1,(T+1))\n",
        "        nanEnd  = np.cumsum(rem1) == np.arange(T,0,-1)\n",
        "        nanLE   = nanLead|nanEnd\n",
        "\n",
        "        # Subsets X\n",
        "        X      = X[~nanLE,:]\n",
        "        indNaN = np.isnan(X) # Index for missing values\n",
        "\n",
        "        for i in range(N): # Loop for each series\n",
        "            x          = X[:,i].copy()\n",
        "            isnanx     = np.isnan(x)\n",
        "            t1         = np.min(np.where(~isnanx)) # First non-NaN entry\n",
        "            t2         = np.max(np.where(~isnanx)) # Last non-NaN entry\n",
        "\n",
        "            # Interpolates without NaN entries in beginning and end\n",
        "            x[t1:t2+1] = CubicSpline(np.where(~isnanx)[0],x[~isnanx])(np.arange(t1,t2+1))\n",
        "            isnanx     = np.isnan(x)\n",
        "\n",
        "            # replace NaN observations with median\n",
        "            x[isnanx]  = np.nanmedian(x)\n",
        "\n",
        "            # Apply filter\n",
        "            x_MA       = lfilter(np.ones((2*k+1))/(2*k+1),1,np.append(np.append(x[0]*np.ones((k,1)),x),x[-1]*np.ones((k,1))))\n",
        "            x_MA       = x_MA[(2*k+1) -1:]\n",
        "\n",
        "            # Replace nanx wih filtered observations\n",
        "            x[isnanx]  = x_MA[isnanx]\n",
        "            X[:,i]     = x\n",
        "\n",
        "    elif options[\"method\"] == 3:\n",
        "        rem1    = np.sum(indNaN, axis = 1) == N\n",
        "        nanLead = np.cumsum(rem1) == np.arange(1,(T+1))\n",
        "        nanEnd  = np.cumsum(rem1) == np.arange(T,0,-1)\n",
        "        nanLE   = nanLead|nanEnd\n",
        "\n",
        "        X      = X[~nanLE,:]\n",
        "        indNaN = np.isnan(X)\n",
        "\n",
        "    elif options[\"method\"] == 4: # remove rows with leading and closing zeros & replace missing values\n",
        "        rem1    = np.sum(indNaN, axis = 1) == N\n",
        "        nanLead = np.cumsum(rem1) == np.arange(1,(T+1))\n",
        "        nanEnd  = np.cumsum(rem1) == np.arange(T,0,-1)\n",
        "        nanLE   = nanLead|nanEnd\n",
        "\n",
        "        X      = X[~nanLE,:]\n",
        "        indNaN = np.isnan(X)\n",
        "\n",
        "        for i in range(N):\n",
        "            x            = X[:, i].copy()\n",
        "            isnanx       = np.isnan(x)\n",
        "            t1           = np.min(np.where(~isnanx))\n",
        "            t2           = np.max(np.where(~isnanx))\n",
        "            x[t1:t2 + 1] = CubicSpline(np.where(~isnanx)[0],x[~isnanx])(np.arange(t1,t2+1))\n",
        "            isnanx       = np.isnan(x)\n",
        "            x[isnanx]    = np.nanmedian(x)\n",
        "            x_MA         = lfilter(np.ones((2 * k + 1)) / (2 * k + 1), 1,\n",
        "                                   np.append(np.append(x[0] * np.ones((k, 1)), x), x[-1] * np.ones((k, 1))))\n",
        "            x_MA      = x_MA[(2 * k + 1) - 1:]\n",
        "            x[isnanx] = x_MA[isnanx]\n",
        "            X[:, i]   = x\n",
        "\n",
        "    elif options[\"method\"] == 5: # replace missing values\n",
        "        indNaN = np.isnan(X)\n",
        "\n",
        "        for i in range(N):\n",
        "            x            = X[:, i].copy()\n",
        "            isnanx       = np.isnan(x)\n",
        "            t1           = np.min(np.where(~isnanx))\n",
        "            t2           = np.max(np.where(~isnanx))\n",
        "            x[t1:t2 + 1] = CubicSpline(np.where(~isnanx)[0],x[~isnanx])(np.arange(t1,t2+1))\n",
        "            isnanx       = np.isnan(x)\n",
        "            x[isnanx]    = np.nanmedian(x)\n",
        "            x_MA         = lfilter(np.ones((2 * k + 1)) / (2 * k + 1), 1,\n",
        "                                   np.append(np.append(x[0] * np.ones((k, 1)), x), x[-1] * np.ones((k, 1))))\n",
        "            x_MA         = x_MA[(2 * k + 1) - 1:]\n",
        "            x[isnanx]    = x_MA[isnanx]\n",
        "            X[:, i]      = x\n",
        "\n",
        "    return X,indNaN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_wQNYEi8jY2"
      },
      "source": [
        "#-------------------------------------------------Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#from remNaNs_spline import remNaNs_spline\n",
        "from scipy.linalg import eig\n",
        "from scipy.linalg import block_diag\n",
        "\n",
        "\n",
        "#-------------------------------------------------Dynamic Factor Modeling functions\n",
        "def dfm(X,Spec,threshold = 1e-5,max_iter = 5000):\n",
        "    # DFM()    Runs the dynamic factor model\n",
        "    #\n",
        "    #  Syntax:\n",
        "    #    Res = DFM(X,Par)\n",
        "    #\n",
        "    #  Description:\n",
        "    #   DFM() inputs the organized and transformed data X and parameter structure Par.\n",
        "    #   Then, the function outputs dynamic factor model structure Res and data\n",
        "    #   summary statistics (mean and standard deviation).\n",
        "    #\n",
        "    #  Input arguments:\n",
        "    #    X: Kalman-smoothed data where missing values are replaced by their expectation\n",
        "    #    Par: A structure containing the following parameters:\n",
        "    #      Par.blocks: Block loadings.\n",
        "    #      Par.nQ: Number of quarterly series\n",
        "    #      Par.p: Number of lags in transition matrix\n",
        "    #      Par.r: Number of common factors for each block\n",
        "    #\n",
        "    # Output Arguments:\n",
        "    #\n",
        "    #   Res - structure of model results with the following fields\n",
        "    #       . X_sm | Kalman-smoothed data where missing values are replaced by their expectation\n",
        "    #       . Z | Smoothed states. Rows give time, and columns are organized according to Res.C.\n",
        "    #       . C | Observation matrix. The rows correspond\n",
        "    #          to each series, and the columns are organized as shown below:\n",
        "    #         - 1-20: These columns give the factor loa dings. For example, 1-5\n",
        "    #              give loadings for the first block and are organized in\n",
        "    #              reverse-chronological order (f^G_t, f^G_t-1, f^G_t-2, f^G_t-3,\n",
        "    #              f^G_t-4). Columns 6-10, 11-15, and 16-20 give loadings for\n",
        "    #              the second, third, and fourth blocks respectively.\n",
        "    #       .R: Covariance for observation matrix residuals\n",
        "    #       .A: Transition matrix. This is a square matrix that follows the\n",
        "    #      same organization scheme as Res.C's columns. Identity matrices are\n",
        "    #      used to account for matching terms on the left and righthand side.\n",
        "    #      For example, we place an I4 matrix to account for matching\n",
        "    #      (f_t-1; f_t-2; f_t-3; f_t-4) terms.\n",
        "    #       .Q: Covariance for transition equation residuals.\n",
        "    #       .Mx: Series mean\n",
        "    #       .Wx: Series standard deviation\n",
        "    #       .Z_0: Initial value of state\n",
        "    #       .V_0: Initial value of covariance matrix\n",
        "    #       .r: Number of common factors for each block\n",
        "    #       .p: Number of lags in transition equation\n",
        "    #\n",
        "    # References:\n",
        "    #\n",
        "    #   Marta Banbura, Domenico Giannone and Lucrezia Reichlin\n",
        "    #   Nowcasting (2010)\n",
        "    #   Michael P. Clements and David F. Hendry, editors,\n",
        "    #   Oxford Handbook on Economic Forecasting.\n",
        "\n",
        "    ## Store model parameters ------------------------------------------------\n",
        "\n",
        "    # DFM input specifications: See documentation for details\n",
        "    Par = {}\n",
        "    Par[\"blocks\"] = Spec.Blocks.copy()                                  # Block loading structure\n",
        "    Par[\"nQ\"]     = (Spec.Frequency == \"q\").sum()                       # Number of quarterly series\n",
        "    Par[\"p\"]      = 1                                                   # Number of lags in autoregressive of factor (same for all factors)\n",
        "    Par[\"r\"]      = np.ones((1,Spec.Blocks.shape[1])).astype(np.int64)  # Number of common factors for each block\n",
        "    # Par.r(1) =2;\n",
        "    # Display blocks\n",
        "\n",
        "    print(\"\\n\\n\\n\")\n",
        "    print(\"Table 3: Block Loading Structure\")\n",
        "    print(pd.DataFrame(data=Spec.Blocks, index=Spec.SeriesName, columns=Spec.BlockNames))\n",
        "    print(\"\\n\")\n",
        "    print(\"Estimating the dynamic factor model (DFM) \\n\\n\")\n",
        "\n",
        "    T, N   = X.shape\n",
        "    r      = Par[\"r\"].copy()\n",
        "    p      = Par[\"p\"]\n",
        "    nQ     = Par[\"nQ\"]\n",
        "    blocks = Par[\"blocks\"].copy()\n",
        "\n",
        "    i_idio = np.append(np.ones(N-nQ),np.zeros(nQ)).reshape((-1,1),order=\"F\") == 1\n",
        "\n",
        "    # R*Lambda = q; Contraints on the loadings of the quartrly variables\n",
        "    R_mat = np.array([2,-1,0,0,0,3,0,-1,0,0,2,0,0,-1,0,1,0,0,0,-1]).reshape((4,5))\n",
        "    q     = np.zeros((4,1))\n",
        "\n",
        "    # Prepare data -----------------------------------------------------------\n",
        "    Mx   = np.nanmean(X,axis = 0)\n",
        "    Wx   = np.nanstd(X,axis = 0,ddof = 1)\n",
        "    xNaN = (X - np.tile(Mx,(T,1))) / np.tile(Wx,(T,1))\n",
        "\n",
        "    # Initial Conditions------------------------------------------------------\n",
        "    optNaN           = {}\n",
        "    optNaN[\"method\"] = 2 # Remove leading and closing zeros\n",
        "    optNaN[\"k\"]      = 3 # Setting for filter(): See remNaN_spline\n",
        "\n",
        "    A,C,Q,R,Z_0,V_0  = InitCond(xNaN.copy(), r.copy(), p, blocks.copy(), optNaN, R_mat.copy(), q, nQ, i_idio.copy())\n",
        "\n",
        "    # initialize EM loop values\n",
        "    previous_loglik = -np.inf\n",
        "    num_iter        = 0\n",
        "    LL              = [-np.inf]\n",
        "    converged       = 0\n",
        "\n",
        "    # y for the estimation is with missing data\n",
        "    y                = xNaN.copy().T\n",
        "\n",
        "    # EM LOOP ----------------------------------------------------------------\n",
        "\n",
        "    # The model can be written as\n",
        "    # y = C*Z + e;\n",
        "    # Z = A*Z(-1) + v\n",
        "    # where y is NxT, Z is (pr)xT, etc\n",
        "\n",
        "    # Remove the leading and ending nans\n",
        "    optNaN[\"method\"] = 3\n",
        "    y_est,_          = remNaNs_spline(xNaN.copy(),optNaN)\n",
        "    y_est            = y_est.T\n",
        "\n",
        "    max_iter = 5000\n",
        "    while num_iter < max_iter and not converged: # Loop until converges or max iter.\n",
        "\n",
        "        # Applying EM algorithm\n",
        "        C_new, R_new, A_new, Q_new, Z_0, V_0, loglik = EMstep(y_est, A, C, Q, R, Z_0, V_0, r,p,R_mat,q,nQ,i_idio,blocks)\n",
        "\n",
        "        C = C_new.copy()\n",
        "        R = R_new.copy()\n",
        "        A = A_new.copy()\n",
        "        Q = Q_new.copy()\n",
        "\n",
        "        if num_iter > 2: # Check convergence\n",
        "            converged, decrease = em_converged(loglik,previous_loglik,threshold,1)\n",
        "\n",
        "        if (num_iter % 10) == 0 and num_iter > 0:\n",
        "            print(\"Now running the {}th iteration of max {}\".format(num_iter,max_iter))\n",
        "            print('Loglik: {} (% Change: {})'.format(loglik, 100*((loglik-previous_loglik)/previous_loglik)))\n",
        "\n",
        "        LL.append(loglik)\n",
        "        previous_loglik = loglik\n",
        "        num_iter        += 1\n",
        "\n",
        "    if num_iter < max_iter:\n",
        "        print('Successful: Convergence at {} interations'.format(num_iter))\n",
        "    else:\n",
        "        print('Stopped because maximum iterations reached')\n",
        "\n",
        "    # Final run of the Kalman filter\n",
        "    Zsmooth,_,_,_  = runKF(y,A,C,Q,R,Z_0,V_0)\n",
        "    Zsmooth        = Zsmooth.T\n",
        "    x_sm           = np.matmul(Zsmooth[1:,:],C.T) # Get smoothed X\n",
        "\n",
        "    # Loading the structure with the results --------------------------------\n",
        "    Res = { \"x_sm\"     : x_sm.copy(),\n",
        "            \"X_sm\" : np.tile(Wx,(T,1)) * x_sm + np.tile(Mx,(T,1)),\n",
        "            \"Z\"        : Zsmooth[1:,:].copy(),\n",
        "            \"C\"        : C.copy(),\n",
        "            \"R\"        : R.copy(),\n",
        "            \"A\"        : A.copy(),\n",
        "            \"Q\"        : Q.copy(),\n",
        "            \"Mx\"       : Mx.copy(),\n",
        "            \"Wx\"       : Wx.copy(),\n",
        "            \"Z_0\"      : Z_0.copy(),\n",
        "            \"V_0\"      : V_0.copy(),\n",
        "            \"r\"        : r,\n",
        "            \"p\"        : p,\n",
        "            \"loglik\"   : LL\n",
        "    }\n",
        "\n",
        "    # Display output\n",
        "    # Table with names and factor loadings\n",
        "    nQ       = Par[\"nQ\"]\n",
        "    nM       = Spec.SeriesID.shape[0] - nQ\n",
        "    nLags    = max(Par[\"p\"],5)\n",
        "    nFactors = np.sum(Par[\"r\"])\n",
        "\n",
        "    print(\"\\n Table 4: Factor Loadings for Monthly Series\")\n",
        "    print(pd.DataFrame(Res[\"C\"][:nM,np.arange(0,nFactors*5,5)],\n",
        "                       columns = Spec.BlockNames,\n",
        "                       index   = Spec.SeriesName[:nM]))\n",
        "\n",
        "    print(\"\\n Table 5: Quarterly Loadings Sample (Global Factor)\")\n",
        "    print(pd.DataFrame(Res[\"C\"][(-1-nQ+1):,:5],\n",
        "                       columns = ['f1_lag0', 'f1_lag1', 'f1_lag2', 'f1_lag3', 'f1_lag4'],\n",
        "                       index   = Spec.SeriesName[-1-nQ+1:]))\n",
        "\n",
        "    # Table with AR model on factors (factors with AR parameter and variance of residuals)\n",
        "    A_terms = np.diag(Res[\"A\"]).copy()\n",
        "    Q_terms = np.diag(Res[\"Q\"]).copy()\n",
        "\n",
        "    print('\\n Table 6: Autoregressive Coefficients on Factors')\n",
        "    print(pd.DataFrame({'AR_Coefficient'    : A_terms[np.arange(0,nFactors*5,5)].copy(),\n",
        "                        'Variance_Residual' : Q_terms[np.arange(0,nFactors*5,5)].copy()},\n",
        "                       index   = Spec.BlockNames))\n",
        "\n",
        "    # Table with AR model idiosyncratic errors (factors with AR parameter and variance of residuals)\n",
        "    print('\\n Table 7: Autoregressive Coefficients on Idiosyncratic Component')\n",
        "    A_len = A.shape[0]\n",
        "    Q_len = Q.shape[0]\n",
        "\n",
        "    A_index = np.hstack([np.arange(nFactors*5,nFactors*5+nM),np.arange(nFactors*5+nM,A_len,5)])\n",
        "    Q_index = np.hstack([np.arange(nFactors*5,nFactors*5+nM),np.arange(nFactors*5+nM,Q_len,5)])\n",
        "\n",
        "    print(pd.DataFrame({'AR_Coefficient'    : A_terms[A_index].copy(),\n",
        "                        'Variance_Residual' : Q_terms[Q_index].copy()},\n",
        "                       index   = Spec.SeriesName))\n",
        "\n",
        "    return Res\n",
        "\n",
        "def InitCond(x,r,p,blocks,optNaN,Rcon,q,nQ,i_idio):\n",
        "    #InitCond()      Calculates initial conditions for parameter estimation\n",
        "    #\n",
        "    #  Description:\n",
        "    #    Given standardized data and model information, InitCond() creates\n",
        "    #    initial parameter estimates. These are intial inputs in the EM\n",
        "    #    algorithm, which re-estimates these parameters using Kalman filtering\n",
        "    #    techniques.\n",
        "    #\n",
        "    #Inputs:\n",
        "    #  - x:      Standardized data\n",
        "    #  - r:      Number of common factors for each block\n",
        "    #  - p:      Number of lags in transition equation\n",
        "    #  - blocks: Gives series loadings\n",
        "    #  - optNaN: Option for missing values in spline. See remNaNs_spline() for details.\n",
        "    #  - Rcon:   Incorporates estimation for quarterly series (i.e. \"tent structure\")\n",
        "    #  - q:      Constraints on loadings for quarterly variables\n",
        "    #  - NQ:     Number of quarterly variables\n",
        "    #  - i_idio: Logical. Gives index for monthly variables (1) and quarterly (0)\n",
        "    #\n",
        "    #Output:\n",
        "    #  - A:   Transition matrix\n",
        "    #  - C:   Observation matrix\n",
        "    #  - Q:   Covariance for transition equation residuals\n",
        "    #  - R:   Covariance for observation equation residuals\n",
        "    #  - Z_0: Initial value of state\n",
        "    #  - V_0: Initial value of covariance matrix\n",
        "\n",
        "    pC  = Rcon.shape[1]   # Gives 'tent' structure size (quarterly to monthly)\n",
        "    ppC = max(p,pC)\n",
        "    n_b = blocks.shape[1] # Number of blocks\n",
        "\n",
        "    xBal,indNaN = remNaNs_spline(x.copy(),optNaN)  # Spline without NaNs\n",
        "    \n",
        "    T,N = xBal.shape  # Time T series number N\n",
        "    nM  = N-nQ        # Number of monthly series\n",
        "    \n",
        "    xNaN         = xBal.copy()\n",
        "    xNaN[indNaN] = np.nan        # Set missing values equal to NaNs\n",
        "    res          = xBal.copy()   # Spline output equal to res Later this is used for residuals\n",
        "    resNaN       = xNaN.copy()   # Later used for residuals\n",
        "\n",
        "    # Initialize model coefficient output\n",
        "    C   = None\n",
        "    A   = None\n",
        "    Q   = None\n",
        "    V_0 = None\n",
        "\n",
        "    # Set the first observations as NaNs: For quarterly-monthly aggreg. scheme\n",
        "    indNaN[:pC-1, :] = np.True_\n",
        "\n",
        "    # Set the first observations as NaNs: For quarterly-monthly aggreg. scheme\n",
        "    for i in range(n_b): # Loop for each block\n",
        "        r_i = r[0,i].copy() # r_i = 1 when block is loaded\n",
        "\n",
        "        # Observation equation -----------------------------------------------\n",
        "\n",
        "        C_i = np.zeros((N, r_i * ppC))     # Initialize state variable matrix helper\n",
        "        idx_i = np.where(blocks[:, i])[0]  # Returns series index loading block i\n",
        "        idx_iM = idx_i[idx_i < nM]         # Monthly series indicies for loaded blocks\n",
        "        idx_iQ = idx_i[idx_i >= nM]        # Quarterly series indicies for loaded blocks\n",
        "\n",
        "        # Returns eigenvector v w/largest eigenvalue d, CHECK: test if eig values are the same in Matlab\n",
        "        d, v = eig(np.cov(res[:, idx_iM], rowvar=False))\n",
        "        e_idx = np.where(d == np.max(d))[0]\n",
        "        d = d[e_idx]\n",
        "        v = v[:, e_idx]\n",
        "\n",
        "        # Flip sign for cleaner output. Gives equivalent results without this section\n",
        "        if np.sum(v) < 0:\n",
        "            v = -v\n",
        "\n",
        "        # For monthly series with loaded blocks (rows), replace with eigenvector\n",
        "        # This gives the loading\n",
        "        C_i[idx_iM,0:r_i] = v.copy()\n",
        "        f                 = np.matmul(res[:,idx_iM],v) # Data projection for eigenvector direction\n",
        "        F                 = np.array(f[(pC-1):f.shape[0],:]).reshape((-1,1))\n",
        "\n",
        "        # Lag matrix using loading. This is later used for quarterly series\n",
        "        for kk in range(1,max(p+1,pC)):\n",
        "            F = np.concatenate((F,f[(pC-1)-kk:f.shape[0]-kk,:]), axis =1)\n",
        "\n",
        "        Rcon_i = np.kron(Rcon,np.eye(r_i)) # Quarterly-monthly aggregation scheme\n",
        "        q_i    = np.kron(q,np.zeros((r_i,1)))\n",
        "\n",
        "        # Produces projected data with lag structure (so pC-1 fewer entries)\n",
        "        ff = F[:, 0:(r_i*pC)].copy()\n",
        "\n",
        "        for j in idx_iQ: # Loop for quarterly variables\n",
        "\n",
        "            # For series j, values are dropped to accommodate lag structure\n",
        "            xx_j = resNaN[(pC-1):,j].copy()\n",
        "\n",
        "            if sum(~np.isnan(xx_j)) < (ff.shape[1] + 2):\n",
        "                xx_j = res[(pC-1):,j].copy()\n",
        "\n",
        "            ff_j = ff[~np.isnan(xx_j),:].copy()\n",
        "            xx_j = xx_j[~np.isnan(xx_j)].reshape((-1,1)).copy()\n",
        "\n",
        "            iff_j = np.linalg.inv(np.matmul(ff_j.T,ff_j))\n",
        "            Cc    = np.matmul(np.matmul(iff_j,ff_j.T),xx_j)\n",
        "\n",
        "            a1 = np.matmul(iff_j,Rcon_i.T)\n",
        "            a2 = np.linalg.inv(np.matmul(np.matmul(Rcon_i,iff_j),Rcon_i.T))\n",
        "            a3 = np.matmul(Rcon_i,Cc)-q_i\n",
        "\n",
        "            # Spline data monthly to quarterly conversion\n",
        "            Cc = Cc - np.matmul(np.matmul(a1,a2),a3)\n",
        "\n",
        "            C_i[j,0:pC*r_i] = Cc.T.copy() # Place in output matrix\n",
        "\n",
        "        # Zeros in first pC-1 entries (replace dropped from lag)\n",
        "        ff = np.concatenate([np.zeros((pC-1,pC*r_i)),ff], axis = 0)\n",
        "\n",
        "        # Residual Calculations\n",
        "        res            = res - np.matmul(ff,C_i.T)\n",
        "        resNaN         = res.copy()\n",
        "        resNaN[indNaN] = np.nan\n",
        "\n",
        "        # Combine past loadings together\n",
        "        if i == 0:\n",
        "            C = C_i.copy()\n",
        "        else:\n",
        "            C = np.hstack([C,C_i.copy()])\n",
        "\n",
        "        # Transition equation ------------------------------------------------\n",
        "\n",
        "        z = F[:,r_i-1].copy()           # Projected data (no lag)\n",
        "        Z = F[:,r_i:(r_i*(p+1))].copy() # Data with lag 1\n",
        "\n",
        "        A_i    = np.zeros((r_i*ppC,r_i*ppC)).T                               # Initialize transition matrix\n",
        "        A_temp = np.matmul(np.matmul(np.linalg.inv(np.matmul(Z.T,Z)),Z.T),z) # OLS: gives coefficient value AR(p) process\n",
        "\n",
        "        A_i[:r_i,:r_i*p]       = A_temp.T.copy()\n",
        "        A_i[r_i:,:r_i*(ppC-1)] = np.eye(r_i*(ppC-1))\n",
        "\n",
        "\n",
        "        Q_i            = np.zeros((ppC*r_i,ppC*r_i))\n",
        "        e              = z - np.matmul(Z,A_temp) # VAR residuals\n",
        "        Q_i[:r_i,:r_i] = np.cov(e, rowvar=False) # VAR covariance matrix\n",
        "\n",
        "        initV_i = np.reshape(np.matmul(np.linalg.inv(np.eye((r_i*ppC)**2) - np.kron(A_i,A_i)),Q_i.flatten('F').reshape((-1,1))),(r_i*ppC,r_i*ppC))\n",
        "\n",
        "        # Gives top left block for the transition matrix\n",
        "        if i == 0:\n",
        "            A   = A_i.copy()\n",
        "            Q   = Q_i.copy()\n",
        "            V_0 = initV_i.copy()\n",
        "        else:\n",
        "            A   = block_diag(A,A_i)\n",
        "            Q   = block_diag(Q,Q_i)\n",
        "            V_0 = block_diag(V_0,initV_i)\n",
        "\n",
        "    eyeN = np.eye(N)[:,i_idio.flatten('F')] # Used inside observation matrix\n",
        "\n",
        "    C    = np.hstack([C,eyeN])\n",
        "    # Monthly-quarterly agreggation scheme\n",
        "    C    = np.hstack([C,np.vstack([np.zeros((nM,5*nQ)),np.kron(np.eye(nQ),np.array([1,2,3,2,1]).reshape((1,-1)))])])\n",
        "    # Initialize covariance matrix for transition matrix\n",
        "    R    = np.diag(np.nanvar(resNaN,ddof = 1,axis = 0))\n",
        "\n",
        "    ii_idio = np.where(i_idio)[0]        # Indicies for monthly variables\n",
        "    n_idio  = ii_idio.shape[0]           # Number of monthly variables\n",
        "    BM      = np.zeros((n_idio,n_idio))  # Initialize monthly transition matrix values\n",
        "    SM      = np.zeros((n_idio, n_idio)) # Initialize monthly residual covariance matrix values\n",
        "\n",
        "    for i in range(n_idio): # Loop for monthly variables\n",
        "\n",
        "        # Set observation equation residual covariance matrix diagonal\n",
        "        R[ii_idio[i],ii_idio[i]] = 1e-4\n",
        "\n",
        "        # Subsetting series residuals for series i\n",
        "        res_i = resNaN[:,ii_idio[i]].copy()\n",
        "\n",
        "        # Returns number of leading/ending zeros\n",
        "        try:\n",
        "            leadZero = np.max(np.where(np.arange(1,T+1) == np.cumsum(np.isnan(res_i)))) + 1\n",
        "        except ValueError:\n",
        "            leadZero = None\n",
        "\n",
        "        try:\n",
        "            endZero  = -(np.max(np.where(np.arange(1,T+1) == np.cumsum(np.isnan(res_i[::-1])))[0]) + 1)\n",
        "        except ValueError:\n",
        "            endZero = None\n",
        "\n",
        "        # Truncate leading and ending zeros\n",
        "        res_i = res[:,ii_idio[i]].copy()\n",
        "        res_i = res_i[:endZero]\n",
        "        res_i = res_i[leadZero:].reshape((-1,1),order=\"F\")\n",
        "\n",
        "        # Linear regression: AR 1 process for monthly series residuals\n",
        "        BM[i,i] = np.matmul(np.matmul(np.linalg.inv(np.matmul(res_i[:-1].T,res_i[:-1])),res_i[:-1].T),res_i[1:])\n",
        "        SM[i,i] = np.cov(res_i[1:] - (res_i[:-1]*BM[i,i]),rowvar=False)\n",
        "\n",
        "    Rdiag       = np.diag(R).copy()\n",
        "    sig_e       = (Rdiag[nM:]/19)\n",
        "    Rdiag[nM:]  = 1e-4\n",
        "    R           = np.diag(Rdiag).copy() # Covariance for obs matrix residuals\n",
        "\n",
        "    # For BQ, SQ\n",
        "    rho0      = np.array([[.1]])\n",
        "    temp      = np.zeros((5,5))\n",
        "    temp[0,0] = 1\n",
        "\n",
        "    # Blocks for covariance matrices\n",
        "    SQ = np.kron(np.diag((1 - rho0[0,0]**2)*sig_e),temp)\n",
        "    BQ = np.kron(np.eye(nQ),np.vstack([np.hstack([rho0,np.zeros((1,4))]),np.hstack([np.eye(4),np.zeros((4,1))])]))\n",
        "\n",
        "    initViQ = np.matmul(np.linalg.inv(np.eye((5*nQ)**2) - np.kron(BQ,BQ)),SQ.reshape((-1,1))).reshape((5*nQ,5*nQ))\n",
        "    initViM = np.diag(1/np.diag(np.eye(BM.shape[0]) - BM**2))*SM\n",
        "\n",
        "    # Output\n",
        "    A   = block_diag(A,BM,BQ)\n",
        "    Q   = block_diag(Q,SM,SQ)\n",
        "    Z_0 = np.zeros((A.shape[0],1))\n",
        "    V_0 = block_diag(V_0,initViM,initViQ)\n",
        "\n",
        "    return A, C, Q, R, Z_0, V_0\n",
        "\n",
        "def EMstep(y, A, C, Q, R, Z_0, V_0, r,p,R_mat,q,nQ,i_idio,blocks):\n",
        "    #EMstep    Applies EM algorithm for parameter reestimation\n",
        "    #\n",
        "    #  Syntax:\n",
        "    #    [C_new, R_new, A_new, Q_new, Z_0, V_0, loglik]\n",
        "    #    = EMstep(y, A, C, Q, R, Z_0, V_0, r, p, R_mat, q, nQ, i_idio, blocks)\n",
        "    #\n",
        "    #  Description:\n",
        "    #    EMstep reestimates parameters based on the Estimation Maximization (EM)\n",
        "    #    algorithm. This is a two-step procedure:\n",
        "    #    (1) E-step: the expectation of the log-likelihood is calculated using\n",
        "    #        previous parameter estimates.\n",
        "    #    (2) M-step: Parameters are re-estimated through the maximisation of\n",
        "    #        the log-likelihood (maximize result from (1)).\n",
        "    #\n",
        "    #    See \"Maximum likelihood estimation of factor models on data sets with\n",
        "    #    arbitrary pattern of missing data\" for details about parameter\n",
        "    #    derivation (Banbura & Modugno, 2010). This procedure is in much the\n",
        "    #    same spirit.\n",
        "    #\n",
        "    #  Input:\n",
        "    #    y:      Series data\n",
        "    #    A:      Transition matrix\n",
        "    #    C:      Observation matrix\n",
        "    #    Q:      Covariance for transition equation residuals\n",
        "    #    R:      Covariance for observation matrix residuals\n",
        "    #    Z_0:    Initial values of factors\n",
        "    #    V_0:    Initial value of factor covariance matrix\n",
        "    #    r:      Number of common factors for each block (e.g. vector [1 1 1 1])\n",
        "    #    p:      Number of lags in transition equation\n",
        "    #    R_mat:  Estimation structure for quarterly variables (i.e. \"tent\")\n",
        "    #    q:      Constraints on loadings\n",
        "    #    nQ:     Number of quarterly series\n",
        "    #    i_idio: Indices for monthly variables\n",
        "    #    blocks: Block structure for each series (i.e. for a series, the structure\n",
        "    #            [1 0 0 1] indicates loadings on the first and fourth factors)\n",
        "    #\n",
        "    #  Output:\n",
        "    #    C_new: Updated observation matrix\n",
        "    #    R_new: Updated covariance matrix for residuals of observation matrix\n",
        "    #    A_new: Updated transition matrix\n",
        "    #    Q_new: Updated covariance matrix for residuals for transition matrix\n",
        "    #    Z_0:   Initial value of state\n",
        "    #    V_0:   Initial value of covariance matrix\n",
        "    #    loglik: Log likelihood\n",
        "    #\n",
        "    # References:\n",
        "    #   \"Maximum likelihood estimation of factor models on data sets with\n",
        "    #   arbitrary pattern of missing data\" by Banbura & Modugno (2010).\n",
        "    #   Abbreviated as BM2010\n",
        "    #\n",
        "    #\n",
        "\n",
        "    # Initialize preliminary values\n",
        "\n",
        "    # Store series/model values\n",
        "    n,T        = y.shape\n",
        "    nM         = n - nQ\n",
        "    pC         = R_mat.shape[1]\n",
        "    ppC        = max(p,pC)\n",
        "    num_blocks = blocks.shape[1]\n",
        "\n",
        "    # ESTIMATION STEP: Compute the (expected) sufficient statistics for a single\n",
        "    # Kalman filter sequence\n",
        "\n",
        "    # Running the Kalman filter and smoother with current parameters\n",
        "    # Note that log-liklihood is NOT re-estimated after the runKF step: This\n",
        "    # effectively gives the previous iteration's log-likelihood\n",
        "    # For more information on output, see runKF\n",
        "    Zsmooth,Vsmooth,VVsmooth,loglik = runKF(y, A, C, Q, R, Z_0, V_0)\n",
        "\n",
        "    # MAXIMIZATION STEP (TRANSITION EQUATION)\n",
        "    # See (Banbura & Modugno, 2010) for details.\n",
        "\n",
        "    # Initialize output\n",
        "    A_new   = A.copy()\n",
        "    Q_new   = Q.copy()\n",
        "    V_0_new = V_0.copy()\n",
        "\n",
        "    # 2A. UPDATE FACTOR PARAMETERS INDIVIDUALLY ----------------------------\n",
        "    for i in range(num_blocks): # Loop for each block: factors are uncorrelated\n",
        "\n",
        "        # SETUP INDEXING\n",
        "        r_i      = r[0,i].copy()         # r_i = 1 if block is loaded\n",
        "        rp       = r_i*p\n",
        "        rp1      = np.sum(r[0,:i]) *ppC\n",
        "        b_subset = np.arange(rp1,rp1+rp) # Subset blocks: Helps for subsetting Zsmooth, Vsmooth\n",
        "        t_start  = rp1                   # Transition matrix factor idx start\n",
        "        t_end    = (rp1+r_i*ppC)         # Transition matrix factor idx end\n",
        "\n",
        "        # ESTIMATE FACTOR PORTION OF Q, A\n",
        "        # Note: EZZ, EZZ_BB, EZZ_FB are parts of equations 6 and 8 in BM 2010\n",
        "\n",
        "        # E[f_t*f_t' | Omega_T]\n",
        "        EZZ    = np.matmul(Zsmooth[b_subset,1:],Zsmooth[b_subset,1:].T) + \\\n",
        "                   np.sum(Vsmooth[1:,[b_subset],[b_subset]], axis =0)\n",
        "\n",
        "        # E[f_{t-1}*f_{t-1}' | Omega_T]\n",
        "        EZZ_BB = np.matmul(Zsmooth[b_subset,:-1],Zsmooth[b_subset,:-1].T) + \\\n",
        "                   np.sum(Vsmooth[:-1,[b_subset],[b_subset]],axis =0)\n",
        "\n",
        "        # E[f_t*f_{t-1}' | Omega_T]\n",
        "        EZZ_FB = np.matmul(Zsmooth[b_subset,1:],Zsmooth[b_subset,:-1].T) + \\\n",
        "                    np.sum(VVsmooth[:,b_subset,b_subset], axis = 0)\n",
        "\n",
        "        # Select transition matrix/covariance matrix for block i\n",
        "        A_i = A[t_start:t_end, t_start:t_end].copy()\n",
        "        Q_i = Q[t_start:t_end, t_start:t_end].copy()\n",
        "\n",
        "        # Equation 6: Estimate VAR(p) for factor\n",
        "        A_i[:r_i,:rp] = np.matmul(EZZ_FB[:r_i,:rp],np.linalg.inv(EZZ_BB[:rp,:rp]))\n",
        "\n",
        "        # Equation 8: Covariance matrix of residuals of VA\n",
        "        Q_i[:r_i,:r_i] = (EZZ[:r_i,:r_i] - np.matmul(A_i[:r_i,:rp],EZZ_FB[:r_i,:rp].T))/T\n",
        "\n",
        "        # Place updated results in output matrix\n",
        "        A_new[t_start:t_end, t_start:t_end]   = A_i.copy()\n",
        "        Q_new[t_start:t_end, t_start:t_end]   = Q_i.copy()\n",
        "        V_0_new[t_start:t_end, t_start:t_end] = Vsmooth[0,t_start:t_end,t_start:t_end].copy()\n",
        "\n",
        "    # B. UPDATING PARAMETERS FOR IDIOSYNCRATIC COMPONENT ------------------\n",
        "\n",
        "    rp1      = np.sum(r)*ppC              # Col size of factor portion\n",
        "    niM      = np.sum(i_idio[:nM])        # Number of monthly values\n",
        "    t_start  = rp1                        # Start of idiosyncratic component index\n",
        "    i_subset = np.arange(t_start,rp1+niM) # Gives indices for monthly idiosyncratic component values\n",
        "\n",
        "    # Below 3 estimate the idiosyncratic component (for eqns 6, 8 BM 2010)\n",
        "\n",
        "    # E[f_t*f_t' | \\Omega_T]\n",
        "    EZZ = np.diag(np.diag(np.matmul(Zsmooth[t_start:,1:],Zsmooth[t_start:,1:].T))) + \\\n",
        "            np.diag(np.diag(np.sum(Vsmooth[1:,t_start:,t_start:], axis = 0)))\n",
        "\n",
        "    # E[f_{t-1}*f_{t-1}' | \\Omega_T]\n",
        "    EZZ_BB = np.diag(np.diag(np.matmul(Zsmooth[t_start:,:-1],Zsmooth[t_start:,:-1].T))) + \\\n",
        "                np.diag(np.diag(np.sum(Vsmooth[:-1,t_start:,t_start:], axis =0)))\n",
        "\n",
        "    # E[f_t*f_{t-1}' | \\Omega_T]\n",
        "    EZZ_FB = np.diag(np.diag(np.matmul(Zsmooth[t_start:,1:], Zsmooth[t_start:,:-1].T))) + \\\n",
        "                np.diag(np.diag(np.sum(VVsmooth[:,t_start:,t_start:], axis = 0)))\n",
        "\n",
        "    A_i = np.matmul(EZZ_FB,np.diag(1/np.diag((EZZ_BB)))) # Equation 6\n",
        "    Q_i = (EZZ - np.matmul(A_i,EZZ_FB.T))/T              # Equation 8\n",
        "\n",
        "    # Place updated results in output matrix\n",
        "    A_new[np.ix_(i_subset,i_subset)]   = A_i[:niM,:niM].copy()\n",
        "    Q_new[np.ix_(i_subset,i_subset)]   = Q_i[:niM,:niM].copy()\n",
        "    V_0_new[np.ix_(i_subset,i_subset)] = np.diag(np.diag(Vsmooth[0,i_subset,i_subset].copy()))\n",
        "\n",
        "    # 3 MAXIMIZATION STEP (observation equation)\n",
        "\n",
        "    # INITIALIZATION AND SETUP ----------------------------------------------\n",
        "\n",
        "    Z_0 = Zsmooth[:,[0]].copy()\n",
        "\n",
        "    # Set missing data series values to 0\n",
        "    y              = y.copy()\n",
        "    nanY           = np.isnan(y).astype(np.int64)\n",
        "    y[np.isnan(y)] = 0\n",
        "\n",
        "    # LOADINGS\n",
        "    C_new = C.copy()\n",
        "\n",
        "    # Blocks\n",
        "    bl   = np.unique(blocks,axis =0) # Gives unique loadings\n",
        "    n_bl = bl.shape[0]               # Number of unique loadings\n",
        "\n",
        "    for i in range(num_blocks): # Loop through each block\n",
        "        if i == 0:\n",
        "            # Initialize indices\n",
        "            bl_idxQ = np.tile(bl[:,[i]],(1,r[0,i]*ppC))\n",
        "            bl_idxM = np.hstack([np.tile(bl[:,[i]],(1,r[0,i])),np.zeros((n_bl,r[0,i]*(ppC-1)))])\n",
        "            R_con   = np.kron(R_mat,np.eye(r[0,i]))\n",
        "            q_con   = np.zeros((r[0,i]*R_mat.shape[0],1))\n",
        "        else:\n",
        "            # Indicator for monthly factor loadings\n",
        "            bl_idxQ = np.hstack([bl_idxQ, np.tile(bl[:,[i]],(1,r[0,i]*ppC))])\n",
        "\n",
        "            # Indicator for quarterly factor loadings\n",
        "            bl_idxM = np.hstack([np.hstack([bl_idxM, np.tile(bl[:,[i]],(1,r[0,i]))]),np.zeros((n_bl,r[0,i]*(ppC-1)))])\n",
        "\n",
        "            # Block diagonal matrix giving monthly-quarterly aggreg scheme\n",
        "            R_con   = block_diag(R_con,np.kron(R_mat,np.eye(r[0,i])))\n",
        "            q_con   = np.vstack([q_con,np.zeros((r[0,i]*R_mat.shape[0],1))])\n",
        "\n",
        "    #  Indicator for monthly/quarterly blocks in observation matrix\n",
        "    bl_idxM = bl_idxM == 1\n",
        "    bl_idxQ = bl_idxQ == 1\n",
        "\n",
        "    i_idio_M = i_idio[:nM].copy()             # Gives 1 for monthly series\n",
        "    n_idio_M = np.where(i_idio_M)[0].shape[0] # Number of monthly series\n",
        "    c_i_idio = np.cumsum(i_idio)              # Cumulative number of monthly series\n",
        "\n",
        "    for i in range(n_bl): # Loop through unique loadings (e.g. [1 0 0 0], [1 1 0 0])\n",
        "\n",
        "        bl_i   = bl[[i],:].copy()\n",
        "        rs     = np.sum(r[np.where(bl_i == 1)])             # Total num of blocks loaded\n",
        "        idx_i  = np.where((blocks == bl_i).all(axis =1))[0] # Indices for bl_i\n",
        "        idx_iM = idx_i[idx_i < nM]                          # Only monthly\n",
        "        n_i    = len(idx_iM)                                # Number of monthly series\n",
        "\n",
        "        # Initialize sums in equation 13 of BGR 2010\n",
        "        denom = np.zeros((n_i*rs,n_i*rs))\n",
        "        nom   = np.zeros((n_i,rs))\n",
        "\n",
        "        # Stores monthly indicies. These are done for input robustness\n",
        "        i_idio_i  = i_idio_M[idx_iM,:].flatten('F').copy()\n",
        "        i_idio_ii = c_i_idio[idx_iM].copy()\n",
        "        i_idio_ii = i_idio_ii[i_idio_i].copy() - 1\n",
        "\n",
        "        # UPDATE MONTHLY VARIABLES: Loop through each period ----------------\n",
        "\n",
        "        # bl_idxM_ind is the same as bl_idxM(i, :) in Matlab\n",
        "        # It can get a bit messy with long indexing\n",
        "        bl_idxM_ind = np.where(bl_idxM[i, :])[0]\n",
        "\n",
        "        for t in range(T):\n",
        "\n",
        "            # Gives selection matrix (1 for nonmissing values)\n",
        "            Wt          = np.diag(np.logical_not(nanY[idx_iM,t]).astype(np.int64))\n",
        "\n",
        "            # E[f_t*t_t' | Omega_T]\n",
        "            denom += np.kron(np.matmul(Zsmooth[bl_idxM_ind][:,[t+1]], Zsmooth[bl_idxM_ind][:,[t+1]].T) + \\\n",
        "                                        Vsmooth[t+1][np.ix_(bl_idxM_ind,bl_idxM_ind)],\n",
        "                                    Wt)\n",
        "\n",
        "            # E[y_t*f_t' | \\Omega_T]\n",
        "            nom += np.matmul(y[idx_iM][:,[t]],Zsmooth[bl_idxM_ind][:,[t+1]].T) - \\\n",
        "                   np.matmul(Wt[:,i_idio_i],\n",
        "                                    np.matmul(Zsmooth[rp1+i_idio_ii][:,[t+1]],Zsmooth[bl_idxM_ind][:,[t+1]].T) + \\\n",
        "                                    Vsmooth[t+1][rp1+i_idio_ii,:][:,bl_idxM_ind])\n",
        "\n",
        "        # POSSIBLE WEAK POINT FOUND: NEED TO TEST ON INDEXING AS NUMPY DOES NOT MAINTAIN PROPER MATRIX FORM DEPENDING ON HOW ITS INDEXED: CHECK\n",
        "\n",
        "        # Eqn 13 BGR 2010\n",
        "        vec_C = np.matmul(np.linalg.inv(denom),nom.flatten('F').reshape((-1,1)))\n",
        "\n",
        "        # Place updated monthly results in output matrix\n",
        "        C_new[np.ix_(idx_iM,bl_idxM_ind)] = vec_C.copy().reshape((n_i,rs),order = \"F\") # CHECK: RESHAPE NEEDS TO BE VERIFIED\n",
        "\n",
        "        # UPDATE QUARTERLY VARIABLES -----------------------------------------\n",
        "\n",
        "        idx_iQ = idx_i[idx_i >=nM].copy() # Index for quarterly series\n",
        "        rps = rs*ppC\n",
        "\n",
        "        # Monthly-quarterly aggregation scheme\n",
        "        R_con_i = R_con[:,bl_idxQ[i,:]]\n",
        "        q_con_i = q_con.copy()\n",
        "\n",
        "        no_c = np.where(~(R_con_i.any(axis = 1)))[0]\n",
        "        R_con_i = np.delete(R_con_i,no_c,axis = 0)\n",
        "        q_con_i = np.delete(q_con_i, no_c, axis=0)\n",
        "\n",
        "        # Loop through quarterly series in loading. This parallels monthly code\n",
        "        for j in idx_iQ:\n",
        "\n",
        "            # Initialization\n",
        "            denom = np.zeros((rps,rps))\n",
        "            nom   = np.zeros((1,rps))\n",
        "\n",
        "            idx_jQ = j - nM # Ordinal position of quarterly variable\n",
        "            # Loc of factor structure corresponding to quarterly var residuals\n",
        "            i_idio_jQ = np.arange(rp1 + n_idio_M + 5*(idx_jQ),rp1 + n_idio_M + 5*(idx_jQ+1))\n",
        "\n",
        "            # Place quarterly values in output matrix\n",
        "            V_0_new[np.ix_(i_idio_jQ,i_idio_jQ)] = Vsmooth[0][np.ix_(i_idio_jQ,i_idio_jQ)].copy()\n",
        "            A_new[i_idio_jQ[0],i_idio_jQ[0]]     = A_i[i_idio_jQ[0]-rp1,i_idio_jQ[0]-rp1].copy()\n",
        "            Q_new[i_idio_jQ[0],i_idio_jQ[0]]     = Q_i[i_idio_jQ[0]-rp1,i_idio_jQ[0]-rp1].copy()\n",
        "\n",
        "            # bl_idxQ_ind is the same as bl_idxQ(i,:) in Matlab\n",
        "            # It can get a bit messy with long indexing\n",
        "            bl_idxQ_ind = np.where(bl_idxQ[i,:])[0]\n",
        "            \n",
        "            for t in range(T):\n",
        "\n",
        "                # Selection matrix for quarterly values\n",
        "                Wt = np.diag(np.logical_not(nanY[[j]][:,[t]]).astype(np.int64))\n",
        "\n",
        "                # Intermediate steps in BGR equation 13\n",
        "                denom += np.kron(np.matmul(Zsmooth[bl_idxQ_ind][:,[t+1]],Zsmooth[bl_idxQ_ind][:,[t+1]].T) + \\\n",
        "                                 Vsmooth[t+1][np.ix_(bl_idxQ_ind,bl_idxQ_ind)],\n",
        "                                 Wt)\n",
        "                nom += y[j,t]*Zsmooth[bl_idxQ_ind,t+1].T\n",
        "                nom -= np.matmul(Wt,np.matmul(np.matmul(np.array([[1,2,3,2,1]]), Zsmooth[i_idio_jQ][:,[t+1]]),\n",
        "                                              Zsmooth[bl_idxQ_ind][:,[t+1]].T) + \\\n",
        "                                    np.matmul(np.array([[1,2,3,2,1]]),Vsmooth[t+1][np.ix_(i_idio_jQ,bl_idxQ_ind)]))\n",
        "\n",
        "            C_i = np.matmul(np.linalg.inv(denom),nom.T)\n",
        "\n",
        "            # BGR equation 13\n",
        "            C_i_constr = C_i - np.matmul(np.matmul(np.matmul(np.linalg.inv(denom),R_con_i.T),\n",
        "                                                   np.linalg.inv(np.matmul(np.matmul(R_con_i,np.linalg.inv(denom)),R_con_i.T))),\n",
        "                                         np.matmul(R_con_i,C_i)-q_con_i)\n",
        "\n",
        "            # Place updated values in output structure\n",
        "            C_new[j,bl_idxQ_ind] = C_i_constr.flatten('F')\n",
        "\n",
        "    # 3B. UPDATE COVARIANCE OF RESIDUALS FOR OBSERVATION EQUATION -----------\n",
        "    # Initialize covariance of residuals of observation equation\n",
        "    R_new = np.zeros((n,n))\n",
        "    for t in range(T):\n",
        "\n",
        "        # Selection matrix\n",
        "        Wt = np.diag(np.logical_not(nanY[:,t])).astype(np.int64)\n",
        "\n",
        "        # BGR equation 15\n",
        "        R_new += np.matmul(y[:,[t]] - np.matmul(np.matmul(Wt,C_new),Zsmooth[:,[t+1]]),\n",
        "                           (y[:,[t]] - np.matmul(np.matmul(Wt,C_new),Zsmooth[:,[t+1]])).T) + \\\n",
        "                 np.matmul(np.matmul(np.matmul(np.matmul(Wt,C_new),Vsmooth[t+1][:,:]),C_new.T),Wt) + \\\n",
        "                 np.matmul(np.matmul((np.eye(n) - Wt),R),(np.eye(n)-Wt))\n",
        "\n",
        "    i_idio_M = np.where(i_idio_M.flatten('F'))[0]\n",
        "    R_new        = R_new/T\n",
        "    RR           = np.diag(R_new).copy() # RR(RR<1e-2) = 1e-2\n",
        "    RR[i_idio_M] = 1e-4                  # Ensure non-zero measurement error. See Doz, Giannone, Reichlin (2012) for reference.\n",
        "    RR[nM:]      = 1e-4\n",
        "    R_new        = np.diag(RR).copy()\n",
        "    # CHECK: np.diag to ensure no read only and\n",
        "    return C_new, R_new, A_new, Q_new, Z_0, V_0, loglik\n",
        "\n",
        "def em_converged(loglik, previous_loglik, threshold = 1e-4, check_decreased = 1):\n",
        "    \n",
        "    # em_converged    checks whether EM algorithm has converged\n",
        "    # \n",
        "    #   Syntax:\n",
        "    #     [converged, decrease] = em_converged(loglik, previous_loglik, threshold, check_increased)\n",
        "    # \n",
        "    #   Description:\n",
        "    #     em_converged() checks whether EM has converged. Convergence occurs if\n",
        "    #     the slope of the log-likelihood function falls below 'threshold'(i.e.\n",
        "    #     f(t) - f(t-1)| / avg < threshold) where avg = (|f(t)| + |f(t-1)|)/2\n",
        "    #     and f(t) is log lik at iteration t. 'threshold' defaults to 1e-4.\n",
        "    # \n",
        "    #     This stopping criterion is from Numerical Recipes in C (pg. 423).\n",
        "    #     With MAP estimation (using priors), the likelihood can decrease\n",
        "    #     even if the mode of the posterior increases.\n",
        "    # \n",
        "    #   Input arguments:\n",
        "    #     loglik: Log-likelihood from current EM iteration\n",
        "    #     previous_loglik: Log-likelihood from previous EM iteration\n",
        "    #     threshold: Convergence threshhold. The default is 1e-4.\n",
        "    #     check_decreased: Returns text output if log-likelihood decreases.\n",
        "    # \n",
        "    #   Output:\n",
        "    #     converged (numeric): Returns 1 if convergence criteria satisfied, and 0 otherwise.\n",
        "    #     decrease (numeric): Returns 1 if loglikelihood decreased.\n",
        "\n",
        "    # Initialize output\n",
        "    converged = 0\n",
        "    decrease  = 0\n",
        "\n",
        "    # Check if log-likelihood decreases (optional)\n",
        "    if check_decreased == 1:\n",
        "        if (loglik - previous_loglik) < -1e-3:\n",
        "            print('******likelihood decreased from {} to {}').format(previous_loglik,loglik)\n",
        "            decrease = 1\n",
        "\n",
        "    # Check convergence criteria\n",
        "    delta_loglik = np.abs(loglik - previous_loglik) # Difference in loglik\n",
        "    avg_loglik   = (np.abs(loglik) + np.abs(previous_loglik) + np.finfo(float).eps)/2\n",
        "\n",
        "    if (delta_loglik/avg_loglik) < threshold:\n",
        "        converged = 1 # Check convergence\n",
        "\n",
        "    return converged, decrease\n",
        "\n",
        "def runKF(Y,A,C,Q,R,Z_0,V_0):\n",
        "    #runKF()    Applies Kalman filter and fixed-interval smoother\n",
        "    #\n",
        "    #  Syntax:\n",
        "    #    [zsmooth, Vsmooth, VVsmooth, loglik] = runKF(Y, A, C, Q, R, Z_0, V_0)\n",
        "    #\n",
        "    #  Description:\n",
        "    #    runKF() applies a Kalman filter and fixed-interval smoother. The\n",
        "    #    script uses the following model:\n",
        "    #           Y_t = C_t Z_t + e_t for e_t ~ N(0, R)\n",
        "    #           Z_t = A Z_{t-1} + mu_t for mu_t ~ N(0, Q)\n",
        "\n",
        "    #  Throughout this file:\n",
        "    #    'm' denotes the number of elements in the state vector Z_t.\n",
        "    #    'k' denotes the number of elements (observed variables) in Y_t.\n",
        "    #    'nobs' denotes the number of time periods for which data are observed.\n",
        "    #\n",
        "    #  Input parameters:\n",
        "    #    Y: k-by-nobs matrix of input data\n",
        "    #    A: m-by-m transition matrix\n",
        "    #    C: k-by-m observation matrix\n",
        "    #    Q: m-by-m covariance matrix for transition equation residuals (mu_t)\n",
        "    #    R: k-by-k covariance for observation matrix residuals (e_t)\n",
        "    #    Z_0: 1-by-m vector, initial value of state\n",
        "    #    V_0: m-by-m matrix, initial value of state covariance matrix\n",
        "    #\n",
        "    #  Output parameters:\n",
        "    #    zsmooth: k-by-(nobs+1) matrix, smoothed factor estimates\n",
        "    #             (i.e. zsmooth(:,t+1) = Z_t|T)\n",
        "    #    Vsmooth: k-by-k-by-(nobs+1) array, smoothed factor covariance matrices\n",
        "    #             (i.e. Vsmooth(:,:,t+1) = Cov(Z_t|T))\n",
        "    #    VVsmooth: k-by-k-by-nobs array, lag 1 factor covariance matrices\n",
        "    #              (i.e. Cov(Z_t,Z_t-1|T))\n",
        "    #    loglik: scalar, log-likelihood\n",
        "    #\n",
        "    #  References:\n",
        "    #  - QuantEcon's \"A First Look at the Kalman Filter\"\n",
        "    #  - Adapted from replication files for:\n",
        "    #    \"Nowcasting\", 2010, (by Marta Banbura, Domenico Giannone and Lucrezia\n",
        "    #    Reichlin), in Michael P. Clements and David F. Hendry, editors, Oxford\n",
        "    #    Handbook on Economic Forecasting.\n",
        "    #\n",
        "    # The software can be freely used in applications.\n",
        "    # Users are kindly requested to add acknowledgements to published work and\n",
        "    # to cite the above reference in any resulting publications\n",
        "\n",
        "    S = SKF(Y, A, C, Q, R, Z_0, V_0)  # Kalman filter\n",
        "    S = FIS(A, S)                     # Fixed interval smoother\n",
        "\n",
        "    # Organize output\n",
        "    zsmooth  = S[\"ZmT\"].copy()\n",
        "    Vsmooth  = S[\"VmT\"].copy()\n",
        "    VVsmooth = S[\"VmT_1\"].copy()\n",
        "    loglik   = S[\"loglik\"].copy()\n",
        "\n",
        "    return zsmooth,Vsmooth,VVsmooth,loglik\n",
        "\n",
        "def SKF(Y, A, C, Q, R, Z_0, V_0):\n",
        "    # SKF    Applies Kalman filter\n",
        "    #\n",
        "    #  Syntax:\n",
        "    #    S = SKF(Y, A, C, Q, R, Z_0, V_0)\n",
        "    #\n",
        "    #  Description:\n",
        "    #    SKF() applies the Kalman filter\n",
        "\n",
        "    #  Input parameters:\n",
        "    #    Y: k-by-nobs matrix of input data\n",
        "    #    A: m-by-m transition matrix\n",
        "    #    C: k-by-m observation matrix\n",
        "    #    Q: m-by-m covariance matrix for transition equation residuals (mu_t)\n",
        "    #    R: k-by-k covariance for observation matrix residuals (e_t)\n",
        "    #    Z_0: 1-by-m vector, initial value of state\n",
        "    #    V_0: m-by-m matrix, initial value of state covariance matrix\n",
        "    #\n",
        "    #  Output parameters:\n",
        "    #    S.Zm: m-by-nobs matrix, prior/predicted factor state vector\n",
        "    #          (S.Zm(:,t) = Z_t|t-1)\n",
        "    #    S.ZmU: m-by-(nobs+1) matrix, posterior/updated state vector\n",
        "    #           (S.Zm(t+1) = Z_t|t)\n",
        "    #    S.Vm: m-by-m-by-nobs array, prior/predicted covariance of factor\n",
        "    #          state vector (S.Vm(:,:,t) = V_t|t-1)\n",
        "    #    S.VmU: m-by-m-by-(nobs+1) array, posterior/updated covariance of\n",
        "    #           factor state vector (S.VmU(:,:,t+1) = V_t|t)\n",
        "    #    S.loglik: scalar, value of likelihood function\n",
        "    #    S.k_t: k-by-m Kalman gain\n",
        "\n",
        "    # INITIALIZE OUTPUT VALUES ---------------------------------------------\n",
        "    # Output structure & dimensions of state space matrix\n",
        "    m    = C.shape[1]\n",
        "\n",
        "    # Outputs time for data matrix. \"number of observations\"\n",
        "    nobs = Y.shape[1]\n",
        "\n",
        "    # Instantiate output\n",
        "    S           = {}\n",
        "    S[\"Zm\"]     = np.zeros((m,nobs))       # Z_t | t-1 (prior)\n",
        "    S[\"Vm\"]     = np.zeros((nobs,m,m))     # V_t | t-1 (prior)\n",
        "    S[\"ZmU\"]    = np.zeros((m,nobs + 1))   # Z_t | t (posterior/updated)\n",
        "    S[\"VmU\"]    = np.zeros((nobs + 1,m,m)) # V_t | t (posterior/updated)\n",
        "    S[\"loglik\"] = 0\n",
        "\n",
        "    # SET INITIAL VALUES ----------------------------------------------------\n",
        "\n",
        "    S[\"Zm\"][:]  = np.nan\n",
        "    S[\"Vm\"][:]  = np.nan\n",
        "    S[\"ZmU\"][:] = np.nan\n",
        "    S[\"VmU\"][:] = np.nan\n",
        "\n",
        "    Zu = Z_0.copy() # Z_0|0 (In below loop, Zu gives Z_t | t)\n",
        "    Vu = V_0.copy() # V_0|0 (In below loop, Vu guvse V_t | t)\n",
        "\n",
        "    # Store initial values\n",
        "    S[\"ZmU\"][:,[0]] = Zu.copy()\n",
        "    S[\"VmU\"][0,:,:] = Vu.copy()\n",
        "\n",
        "    # KALMAN FILTER PROCEDURE ----------------------------------------------\n",
        "    for t in range(nobs):\n",
        "        # CALCULATING PRIOR DISTIBUTION----------------------------------\n",
        "\n",
        "        # Use transition eqn to create prior estimate for factor\n",
        "        # i.e. Z = Z_t|t-1\n",
        "        Z = np.matmul(A,Zu)\n",
        "\n",
        "        # Prior covariance matrix of Z (i.e. V = V_t|t-1)\n",
        "        # Var(Z) = Var(A*Z + u_t) = Var(A*Z) + Var(\\epsilon) =\n",
        "        # A*Vu*A' + Q\n",
        "        V = np.matmul(np.matmul(A,Vu),A.T) + Q\n",
        "        V = .5 * (V + V.T) # Trick to make symmetric\n",
        "\n",
        "        # CALCULATING POSTERIOR DISTRIBUTION ----------------------------\n",
        "\n",
        "        # Removes missing series: These are removed from Y, C, and R\n",
        "        Y_t,C_t,R_t,_ = MissData(Y[:,[t]],C,R)\n",
        "\n",
        "        # Check if y_t contains no data. If so, replace Zu and Vu with prior.\n",
        "        if Y_t.shape[0] == 0:\n",
        "            Zu = Z.copy()\n",
        "            Vu = V.copy()\n",
        "        else:\n",
        "            # Steps for variance and population regression coefficients:\n",
        "            # Var(c_t*Z_t + e_t) = c_t Var(A) c_t' + Var(u) = c_t*V *c_t' + R\n",
        "            VC = np.matmul(V,C_t.T)\n",
        "            iF = np.linalg.inv(np.matmul(C_t,VC) + R_t)\n",
        "\n",
        "            # Matrix of population regression coefficients (QuantEcon eqn #4)\n",
        "            VCF = np.matmul(VC,iF)\n",
        "\n",
        "            # Gives difference between actual and predicted observation\n",
        "            # matrix values\n",
        "            innov = Y_t - np.matmul(C_t,Z)\n",
        "\n",
        "            # Update estimate of factor values (posterior)\n",
        "            Zu = Z + np.matmul(VCF,innov)\n",
        "\n",
        "            # Update covariance matrix (posterior) for time t\n",
        "            Vu = V - np.matmul(VCF,VC.T)\n",
        "            Vu = .5 * (Vu + Vu.T)\n",
        "\n",
        "            # Update log likelihood\n",
        "            S[\"loglik\"] = S[\"loglik\"] + .5*(np.log(np.linalg.det(iF)) - np.matmul(np.matmul(innov.T,iF), innov))[0,0]\n",
        "\n",
        "        # STORE OUTPUT----------------------------------------------------\n",
        "\n",
        "        # Store covariance and observation values for t-1 (priors)\n",
        "        S[\"Zm\"][:,[t]]   = Z.copy()\n",
        "        S[\"Vm\"][[t],:,:] = V.copy()\n",
        "\n",
        "        # Store covariance and state values for t (posteriors)\n",
        "        # i.e. Zu = Z_t|t   & Vu = V_t|t\n",
        "        S[\"ZmU\"][:,[t+1]]   = Zu.copy()\n",
        "        S[\"VmU\"][t+1,:,:] = Vu.copy()\n",
        "\n",
        "    # Store Kalman gain k_t\n",
        "    if Y_t.shape[0] == 0:\n",
        "        S[\"k_t\"] = np.zeros((m,m))\n",
        "    else:\n",
        "        S[\"k_t\"] = np.matmul(VCF,C_t)\n",
        "\n",
        "    return S\n",
        "\n",
        "def FIS(A,S):\n",
        "    #FIS()    Applies fixed-interval smoother\n",
        "    #\n",
        "    #  Syntax:\n",
        "    #    S = FIS(A, S)\n",
        "    #\n",
        "    #  Description:\n",
        "    #    SKF() applies a fixed-interval smoother, and is used in conjunction\n",
        "    #    with SKF(). See  page 154 of 'Forecasting, structural time series models\n",
        "    #    and the Kalman filter' for more details (Harvey, 1990).\n",
        "    #\n",
        "    #  Input parameters:\n",
        "    #    A: m-by-m transition matrix\n",
        "    #    S: structure returned by SKF()\n",
        "    #\n",
        "    #  Output parameters:\n",
        "    #    S: FIS() adds the following smoothed estimates to the S structure:\n",
        "    #    - S.ZmT: m-by-(nobs+1) matrix, smoothed states\n",
        "    #             (S.ZmT(:,t+1) = Z_t|T)\n",
        "    #    - S.VmT: m-by-m-by-(nobs+1) array, smoothed factor covariance\n",
        "    #             matrices (S.VmT(:,:,t+1) = V_t|T = Cov(Z_t|T))\n",
        "    #    - S.VmT_1: m-by-m-by-nobs array, smoothed lag 1 factor covariance\n",
        "    #               matrices (S.VmT_1(:,:,t) = Cov(Z_t Z_t-1|T))\n",
        "    #\n",
        "    #  Model:\n",
        "    #   Y_t = C_t Z_t + e_t for e_t ~ N(0, R)\n",
        "    #   Z_t = A Z_{t-1} + mu_t for mu_t ~ N(0, Q)\n",
        "\n",
        "    # ORGANIZE INPUT ---------------------------------------------------------\n",
        "\n",
        "    # Initialize output matrices\n",
        "    m,nobs   = S[\"Zm\"].shape\n",
        "    S[\"ZmT\"] = np.zeros((m,nobs+1))\n",
        "    S[\"VmT\"] = np.zeros((nobs+1,m,m))\n",
        "\n",
        "    # Fill the final period of ZmT, VmT with SKF() posterior values\n",
        "    S[\"ZmT\"][:,nobs]   = np.squeeze(S[\"ZmU\"][:,nobs])\n",
        "    S[\"VmT\"][nobs,:,:] = np.squeeze(S[\"VmU\"][nobs,:,:])\n",
        "\n",
        "    # Initialize VmT_1 lag 1 covariance matrix for final period\n",
        "    VmT_1_init             = np.matmul(np.matmul(np.eye(m) - S[\"k_t\"],A),np.squeeze(S[\"VmU\"][nobs-1,:,:]))\n",
        "    S[\"VmT_1\"]             = np.zeros((nobs,VmT_1_init.shape[0],VmT_1_init.shape[1]))\n",
        "    S[\"VmT_1\"][nobs-1,:,:] = VmT_1_init\n",
        "\n",
        "    # Used for recursion process. See companion file for details\n",
        "    J_2 = np.matmul(np.matmul(np.squeeze(S[\"VmU\"][nobs-1,:,:]),A.T),np.linalg.pinv(np.squeeze(S[\"Vm\"][nobs-1,:,:])))\n",
        "\n",
        "    # RUN SMOOTHING ALGORITHM ----------------------------------------------\n",
        "    for t in range(nobs)[::-1]: # Loop through time reverse-chronologically (starting at final period nobs)\n",
        "\n",
        "        # Store posterior and prior factor covariance values\n",
        "        VmU = np.squeeze(S[\"VmU\"][t,:,:])\n",
        "        Vm1 = np.squeeze(S[\"Vm\"][t,:,:])\n",
        "\n",
        "        # Store previous period smoothed factor covariance and lag-1 covariance\n",
        "        V_T  = np.squeeze(S[\"VmT\"][t+1,:,:])\n",
        "        V_T1 = np.squeeze(S[\"VmT_1\"][t,:,:])\n",
        "\n",
        "        J_1 = J_2.copy()\n",
        "\n",
        "        # Update smoothed factor estimate\n",
        "        S[\"ZmT\"][:,[t]] = S[\"ZmU\"][:,[t]] + np.matmul(J_1,S[\"ZmT\"][:,[t+1]] - np.matmul(A,S[\"ZmU\"][:,[t]]))\n",
        "\n",
        "        # Update smoothed factor covariance matrix\n",
        "        S[\"VmT\"][t,:,:] = VmU + np.matmul(J_1,np.matmul((V_T - Vm1),J_1.T))\n",
        "\n",
        "        if t>0:\n",
        "            # Update weight\n",
        "            J_2  = np.matmul(np.matmul(np.squeeze(S[\"VmU\"][t-1,:,:]),A.T),np.linalg.pinv(np.squeeze(S[\"Vm\"][t-1,:,:])))\n",
        "\n",
        "            # Update lag 1 factor covariance matrix\n",
        "            S[\"VmT_1\"][t-1,:,:] = np.matmul(VmU,J_2.T) + np.matmul(J_1,np.matmul(V_T1 - np.matmul(A,VmU),J_2.T))\n",
        "    return S\n",
        "\n",
        "def MissData(y,C,R):\n",
        "    # Syntax:\n",
        "    # Description:\n",
        "    #   Eliminates the rows in y & matrices C, R that correspond to missing\n",
        "    #   data (NaN) in y\n",
        "    #\n",
        "    # Input:\n",
        "    #   y: Vector of observations at time t\n",
        "    #   C: Observation matrix\n",
        "    #   R: Covariance for observation matrix residuals\n",
        "    #\n",
        "    # Output:\n",
        "    #   y: Vector of observations at time t (reduced)\n",
        "    #   C: Observation matrix (reduced)\n",
        "    #   R: Covariance for observation matrix residuals\n",
        "    #   L: Used to restore standard dimensions(n x #) where # is the nr of\n",
        "    #      available data in y\n",
        "\n",
        "    # Returns 1 for nonmissing series\n",
        "    ix = np.where(~np.isnan(y).flatten('F'))[0]\n",
        "\n",
        "    # Index for columns with nonmissing variables\n",
        "    e  = np.eye(y.shape[0])\n",
        "    L  = e[:,ix].copy()\n",
        "\n",
        "    # Removes missing series\n",
        "    y  = y[ix].copy()\n",
        "\n",
        "    # Removes missing series from observation matrix\n",
        "    C  =  C[ix,:].copy()\n",
        "\n",
        "    # Removes missing series from transition matrix\n",
        "    R  =  R[np.ix_(ix,ix)].copy()\n",
        "\n",
        "    return y,C,R,L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DxfTcnJ-q4p"
      },
      "source": [
        "# Summarize (summarize.py)\n",
        "\n",
        "#-------------------------------------------------Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime as dt\n",
        "\n",
        "\n",
        "#-------------------------------------------------Functions\n",
        "def summarize(X, Time, Spec):\n",
        "    \"\"\"\n",
        "    summarize Display the detail table for data entering the DFM\n",
        "\n",
        "    Description:\n",
        "        Display the detail table for the nowcast, decomposing nowcast changes\n",
        "        into news and impacts for released data series.\n",
        "    \"\"\"\n",
        "    print(\"\\n\\n\\n\")\n",
        "    print('Table 2: Data Summary \\n')\n",
        "\n",
        "    # Number of rows and data series\n",
        "    T,N = X.shape\n",
        "\n",
        "    print(\"N =     {} data series\".format(N))\n",
        "    print(\"T =     {} observations from {} to {} \\n\".format(T,\n",
        "                                                         dt.fromordinal(Time[0] - 366).strftime('%Y-%m-%d'),\n",
        "                                                         dt.fromordinal(Time[-1] - 366).strftime('%Y-%m-%d')))\n",
        "\n",
        "    # create base table to add additional columns\n",
        "    base = pd.DataFrame(X, columns=Spec.SeriesName)\n",
        "\n",
        "    # Create additional columns\n",
        "    time_range    = base.apply(lambda x: data_table_prep(1,x,Time = Time)).values\n",
        "    max_min_range = base.apply(lambda x: data_table_prep(2,x,Time = Time)).values\n",
        "    Frequency     = data_table_prep(3,freq_dict={'m':\"Monthly\", \"q\":\"Quarterly\"},Spec=Spec)\n",
        "    Units         = data_table_prep(4, N=N,Spec=Spec)\n",
        "\n",
        "    # Transform base dataframe\n",
        "    base = base.describe().T\n",
        "    base[\"Observation Time Range\"] = time_range\n",
        "    base[\"Min/Max Dates\"] = max_min_range\n",
        "    base[\"Frequency\"] = Frequency\n",
        "    base[\"Units\"] = Units\n",
        "\n",
        "    # Rename columns and set the order of columns\n",
        "    base.rename(columns={\"count\": \"Observations\"}, inplace=True)\n",
        "    orderColumns = [\"Observations\",\n",
        "                    \"Observation Time Range\",\n",
        "                    \"Units\",\n",
        "                    \"Frequency\",\n",
        "                    \"mean\",\n",
        "                    \"std\",\n",
        "                    \"min\",\n",
        "                    \"Min/Max Dates\"]\n",
        "\n",
        "    # Rename index values of dataframe\n",
        "    base.rename(index = {base.index[i]:base.index[i] + \" [{}]\".format(Spec.SeriesID[i])\n",
        "                         for i in range(N)},\n",
        "                inplace = True)\n",
        "\n",
        "    # Print dataframe\n",
        "    print(base[orderColumns])\n",
        "\n",
        "def data_table_prep(option,x=None,freq_dict=None,Time=None,N=None,Spec=None):\n",
        "    \"\"\"\n",
        "    The param option takes values from 1 to 4\n",
        "\n",
        "    Option 1:\n",
        "        :param x: a pd.Series\n",
        "        :param Time: an array containing ordinal values\n",
        "\n",
        "        1.) Find values that are not NA\n",
        "        2.) Do a cumsum\n",
        "        3.) Find the first value that is not NA (will be 1)\n",
        "        4.) Find the max value\n",
        "        5.) First value and max value are the first and last date\n",
        "        :return: a string containing the start and last date values that are not NA\n",
        "\n",
        "    Option 2:\n",
        "        :param x: a pd.Series\n",
        "        :param Time: an array containing ordinal values\n",
        "\n",
        "        1.) Get the index of the min and max values of x\n",
        "        2.) Get the Time value in respect to the two indices\n",
        "        :return: a string containing dates that represent min and max values\n",
        "\n",
        "    Option 3:\n",
        "        :param freq_dict: dictionary containing abbreviated as keys with values that are full names: e.g. \"q\" -> \"Quarterly\"\n",
        "        1.) Convert the abbreviated form of frequency to full name\n",
        "         :return: a string containing full name of the frequency\n",
        "\n",
        "    Option 4:\n",
        "        :param N: Number of data series\n",
        "        :param Spec: Spec class object\n",
        "        :return: returns a string representing unit transformed\n",
        "    \"\"\"\n",
        "\n",
        "    if option == 1:\n",
        "        truth   = (np.isnan(x) == False).cumsum()\n",
        "        first   = Time[np.where(truth == 1)[0][0]]\n",
        "        last    = Time[np.max(truth) - 1] # subtracted by 1 to get index value\n",
        "        return \"{} to {}\".format(dt.fromordinal(first - 366).strftime('%b-%Y'),\n",
        "                                dt.fromordinal(last - 366).strftime('%b-%Y'))\n",
        "    elif option == 2:\n",
        "        min = Time[np.where(x == np.min(x))[0][0]]\n",
        "        max = Time[np.where(x == np.max(x))[0][0]]\n",
        "\n",
        "        return \"{} / {}\".format(dt.fromordinal(min - 366).strftime('%b-%Y'),\n",
        "                                dt.fromordinal(max - 366).strftime('%b-%Y'))\n",
        "\n",
        "    elif option == 3:\n",
        "        return [freq_dict[i] for i in Spec.Frequency]\n",
        "\n",
        "    elif option == 4:\n",
        "        return [unitTransformed(Spec.Units[i],\n",
        "                                Spec.Transformation[i],\n",
        "                                Spec.Frequency[i])\n",
        "                for i in range(N)]\n",
        "\n",
        "    else:\n",
        "        ValueError(\"Option needed: must be 1 or 2\")\n",
        "\n",
        "def unitTransformed(unit,transform,freq):\n",
        "    \"\"\"\n",
        "    :param unit: a data series' unit type\n",
        "    :param transform: what transformation was applied to the data series\n",
        "    :param freq: the frequency of the data series\n",
        "    :return: returns a string representing unit transformed\n",
        "    \"\"\"\n",
        "    if unit == \"Index\":\n",
        "        unit_transformed = \"Index\"\n",
        "\n",
        "    elif transform == \"chg\":\n",
        "        if \"%\" in unit:\n",
        "            unit_transformed = \"Ppt. change\"\n",
        "        else:\n",
        "            unit_transformed = \"Level change\"\n",
        "\n",
        "    elif \"pch\" == transform and freq == \"m\":\n",
        "        unit_transformed = \"MoM %\"\n",
        "    elif \"pca\" == transform and freq == \"q\":\n",
        "        unit_transformed = \"QoQ AR %\"\n",
        "    else:\n",
        "        unit_transformed = unit + \" [{}]\".format(transform)\n",
        "\n",
        "    return unit_transformed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk4SwjQHV-RU"
      },
      "source": [
        "#-------------------------------------------------Libraries\n",
        "from datetime import datetime as dt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from dateutil.relativedelta import relativedelta\n",
        "#from Functions.dfm import SKF,FIS\n",
        "\n",
        "\n",
        "#-------------------------------------------------update_nowcast\n",
        "def update_nowcast(X_old,X_new,Time,Spec,Res,series,period,vintage_old,vintage_new):\n",
        "\n",
        "    if not type(vintage_old) == type(0):\n",
        "        vintage_old = dt.strptime(vintage_old, '%Y-%m-%d').date().toordinal() + 366\n",
        "\n",
        "    if not type(vintage_new) == type(0):\n",
        "        vintage_new = dt.strptime(vintage_new, '%Y-%m-%d').date().toordinal() + 366\n",
        "\n",
        "    # Make sure datasets are the same size\n",
        "    N     = np.shape(X_new)[1]\n",
        "\n",
        "    # append 1 year (12 months) of data to each dataset to allow for\n",
        "    # forecasting at different horizons\n",
        "    T_old = np.shape(X_old)[0]\n",
        "    T_new = np.shape(X_new)[0]\n",
        "\n",
        "    if T_new > T_old:\n",
        "\n",
        "        temp    = np.zeros((T_new - T_old,N))\n",
        "        temp[:] = np.nan\n",
        "        X_old   = np.vstack([X_old,temp])\n",
        "\n",
        "    temp    = np.zeros((12, N))\n",
        "    temp[:] = np.nan\n",
        "\n",
        "    # append 1 year (12 months) of data to each dataset to allow for\n",
        "    # forecasting at different horizons\n",
        "    X_old = np.vstack([X_old, temp])\n",
        "    X_new = np.vstack([X_new, temp])\n",
        "\n",
        "    future = np.array([(dt.fromordinal(Time[-1] - 366) +\n",
        "                           relativedelta(months=+ i)).toordinal() + 366 for i in range(1,13)])\n",
        "\n",
        "    Time = np.hstack([Time,future])\n",
        "\n",
        "    i_series    = np.where(series == Spec.SeriesID)[0]\n",
        "    series_name = Spec.SeriesName[i_series]\n",
        "    freq        = Spec.Frequency[i_series][0]\n",
        "\n",
        "    if freq == 'm':\n",
        "        y, m      = period.split(freq)\n",
        "        y         = int(y)\n",
        "        m         = int(m)\n",
        "        d         = 1\n",
        "        t_nowcast = np.where((dt(y,m,d).toordinal()+366) == Time)[0]\n",
        "\n",
        "    elif freq == 'q':\n",
        "        y, q      = period.split(freq)\n",
        "        y         = int(y)\n",
        "        m         = 3 * int(q)\n",
        "        d         = 1\n",
        "        t_nowcast = np.where((dt(y,m,d).toordinal()+366) == Time)[0]\n",
        "\n",
        "    else:\n",
        "        return \"Freq's value is not appropiate\"\n",
        "\n",
        "    if t_nowcast.size == 0:\n",
        "        ValueError('Period is out of nowcasting horizon (up to one year ahead).')\n",
        "\n",
        "    # Update nowcast for target variable 'series' (i) at horizon 'period' (t)\n",
        "    #   > Relate nowcast update into news from data releases:\n",
        "    #     a. Compute the impact from data revisions\n",
        "    #     b. Compute the impact from new data releases\n",
        "\n",
        "    X_rev                  = X_new.copy()\n",
        "    X_rev[np.isnan(X_old)] = np.nan\n",
        "\n",
        "    # Compute news --------------------------------------------------------\n",
        "\n",
        "    # Compute impact from data revisions\n",
        "    y_old,_,_,_,_,_,_,_,_ = News_DFM(X_old, X_rev, Res, t_nowcast, i_series)\n",
        "\n",
        "    # Display output\n",
        "    y_rev,y_new,_,actual,forecast,weight,_,_,_ = News_DFM(X_rev,X_new,Res,t_nowcast,i_series)\n",
        "\n",
        "    print(\"\\n Nowcast Update: {}\".format(dt.fromordinal(vintage_new-366).isoformat().split('T')[0]))\n",
        "    print(\"\\n Nowcast for: {} ({}), {}\".format(Spec.SeriesName[i_series][0],\n",
        "                                               Spec.UnitsTransformed[i_series][0],\n",
        "                                               pd.to_datetime(dt.fromordinal(Time[t_nowcast]-366)).to_period('Q')))\n",
        "\n",
        "    # Only display table output if a forecast is made\n",
        "    if forecast.shape[0] == 0:\n",
        "        print(\"\\n No forecast was made \\n\")\n",
        "    else:\n",
        "        impact_revisions = y_rev - y_old     # Impact from revisions\n",
        "        news             = actual - forecast # News from releases\n",
        "        impact_releases  = weight * news     # Impact of releases\n",
        "\n",
        "        # Store results\n",
        "        news_table = pd.DataFrame({'Forecast': forecast.flatten('F'),\n",
        "                                   'Actual'  : actual.flatten('F'),\n",
        "                                   'Weight'  : weight.flatten('F'),\n",
        "                                   'Impact'  : impact_releases.flatten('F')},\n",
        "                                  index = Spec.SeriesID)\n",
        "\n",
        "        # Select only series with updates\n",
        "        data_released = np.any(np.isnan(X_old) & ~np.isnan(X_new),0)\n",
        "\n",
        "        # Display the impact decomposition\n",
        "        print('\\n Nowcast Impact Decomposition')\n",
        "        print(' Note: The displayed output is subject to rounding error\\n\\n')\n",
        "        print('              {} nowcast:              {:.5f}'.format(dt.fromordinal(vintage_old-366).isoformat().split('T')[0],y_old[0]))\n",
        "        print('      Impact from data revisions:      {:.5f}'.format(impact_revisions[0]))\n",
        "        print('       Impact from data releases:      {:.5f}'.format(np.nansum(news_table.Impact)))\n",
        "        print('                                     +_________')\n",
        "        print('                    Total impact:      {:.5f}'.format(impact_revisions[0] + np.nansum(news_table.Impact)))\n",
        "        print('              {} nowcast:              {:.5f}'.format(dt.fromordinal(vintage_new-366).isoformat().split('T')[0],\n",
        "                                                                                y_new[0]))\n",
        "\n",
        "        print('\\n  Nowcast Detail Table \\n')\n",
        "        print(news_table.iloc[np.where(data_released)[0],:])\n",
        "\n",
        "def News_DFM(X_old,X_new,Res,t_fcst,v_news):\n",
        "    # News_DFM()    Calculates changes in news\n",
        "    # Syntax:\n",
        "    # [y_old, y_new, singlenews, actual, fore, weight ,t_miss, v_miss, innov] = ...\n",
        "    #   News_DFM(X_old, X_new, Q, t_fcst, v_news)\n",
        "    #\n",
        "    # Description:\n",
        "    #  News DFM() inputs two datasets, DFM parameters, target time index, and\n",
        "    #  target variable index. The function then produces Nowcast updates and\n",
        "    #  decomposes the changes into news.\n",
        "    #\n",
        "    # Input Arguments:\n",
        "    #   X_old:  Old data matrix (old vintage)\n",
        "    #   X_new:  New data matrix (new vintage)\n",
        "    #   Res:    DFM() output results (see DFM for more details)\n",
        "    #   t_fcst: Index for target time\n",
        "    #   v_news: Index for target variable\n",
        "    #\n",
        "    # Output Arguments:\n",
        "    #   y_old:       Old nowcast\n",
        "    #   y_new:       New nowcast\n",
        "    #   single_news: News for each data series\n",
        "    #   actual:      Observed series release values\n",
        "    #   fore:        Forecasted series values\n",
        "    #   weight:      News weight\n",
        "    #   t_miss:      Time index for data releases\n",
        "    #   v_miss:      Series index for data releases\n",
        "    #   innov:       Difference between observed and predicted series values (\"innovation\")\n",
        "\n",
        "    # Initialize variables\n",
        "    r          = Res[\"C\"].shape[1]\n",
        "    N          = X_new.shape[1]\n",
        "    singlenews = np.zeros((1,N)) # Initialize news vector (will store news for each series)\n",
        "\n",
        "\n",
        "    # NO FORECAST CASE: Already values for variables v_news at time t_fcst\n",
        "    if ~np.isnan(X_new[t_fcst,v_news])[0]:\n",
        "\n",
        "        Res_old = para_const(X_old,Res,0) # Apply Kalman filter for old data\n",
        "\n",
        "        y_old   = np.zeros((1,v_news.shape[0]))\n",
        "        y_new   = np.zeros((1, v_news.shape[0]))\n",
        "        for i in range(v_news.shape[0]): # Loop for each target variable\n",
        "\n",
        "            # (Observed value) - (predicted value)\n",
        "            singlenews[:,v_news[i]] = X_new[t_fcst,v_news[i]] - Res_old[\"X_sm\"][t_fcst,v_news[i]]\n",
        "\n",
        "            # Set predicted and observed y values\n",
        "            y_old[0,i] = Res_old[\"X_sm\"][t_fcst,v_news[i]].copy()\n",
        "            y_new[0,i] = X_new[t_fcst, v_news[i]].copy()\n",
        "\n",
        "        # Forecast-related output set to empty\n",
        "        return y_old,y_new,singlenews,None,None,None,None,None,None\n",
        "\n",
        "    else:\n",
        "        # FORECAST CASE (these are broken down into (A) and (B))\n",
        "\n",
        "        # Initialize series mean/standard deviation respectively\n",
        "        Mx = Res[\"Mx\"].reshape((-1,1))\n",
        "        Wx = Res[\"Wx\"].reshape((-1,1))\n",
        "\n",
        "        # Calculate indicators for missing values (1 if missing, 0 otherwise)\n",
        "        miss_old = np.isnan(X_old).astype(np.int64)\n",
        "        miss_new = np.isnan(X_new).astype(np.int64)\n",
        "\n",
        "        # Indicator for missing--combine above information to single matrix where:\n",
        "        # (i) -1: Value is in the old data, but missing in new data\n",
        "        # (ii) 1: Value is in the new data, but missing in old data\n",
        "        # (iii) 0: Values are missing from/available in both datasets\n",
        "        i_miss = miss_old - miss_new\n",
        "\n",
        "        # Time/variable indicies where case (b) is true\n",
        "        t_miss, v_miss = np.where(i_miss == 1)\n",
        "        ordered_col    = v_miss.argsort()\n",
        "        t_miss, v_miss = t_miss[ordered_col], v_miss[ordered_col]\n",
        "\n",
        "        # FORECAST SUBCASE (A): NO NEW INFORMATION\n",
        "        if v_miss.shape[0] == 0:\n",
        "\n",
        "            # Fill in missing variables using a Kalman filter\n",
        "            Res_old = para_const(X_old, Res, 0)\n",
        "            Res_new = para_const(X_new, Res, 0) # CHECK: Why isn't this being used?\n",
        "\n",
        "            # Set predicted and observed y values. New y value is set to old\n",
        "            y_old = Res_old[\"X_sm\"][t_fcst,v_news]\n",
        "            y_new = y_old.copy()\n",
        "\n",
        "            # No news, so nothing returned for news-related output\n",
        "            return y_old,y_new,singlenews,None,None,None,None,None,None\n",
        "\n",
        "        else:\n",
        "            #----------------------------------------------------------------------\n",
        "            #     v_miss=[1:size(X_new,2)]';\n",
        "            #     t_miss=t_miss(1)*ones(size(X_new,2),1);\n",
        "            #----------------------------------------------------------------------\n",
        "            # FORECAST SUBCASE (B): NEW INFORMATION\n",
        "\n",
        "            # Difference between forecast time and new data time\n",
        "            lag = t_fcst - t_miss\n",
        "\n",
        "            # Gives biggest time interval between forecast and new data\n",
        "            k   = np.max(np.hstack([np.abs(lag),np.max(lag) - np.min(lag)]))\n",
        "\n",
        "            C = Res[\"C\"].copy() # Observation matrix\n",
        "            R = Res[\"R\"].copy() # Covariance for observation matrix residuals\n",
        "\n",
        "            # Number of new events\n",
        "            n_news = lag.shape[0]\n",
        "\n",
        "            # Smooth old dataset\n",
        "            Res_old = para_const(X_old, Res, k)\n",
        "            Plag    = Res_old[\"Plag\"].copy()\n",
        "\n",
        "            # Smooth new dataset\n",
        "            Res_new = para_const(X_new, Res, 0)\n",
        "\n",
        "            # Subset for target variable and forecast time\n",
        "            y_old = Res_old[\"X_sm\"][t_fcst,v_news]\n",
        "            y_new = Res_new[\"X_sm\"][t_fcst,v_news]\n",
        "\n",
        "            P  = Res_old[\"P\"][1:].copy()\n",
        "\n",
        "            for i in range(n_news): # Cycle through total number of updates\n",
        "                h = abs(t_fcst-t_miss[i])[0]\n",
        "                m = np.maximum(t_miss[i],t_fcst)[0]\n",
        "\n",
        "                # If location of update is later than the forecasting date\n",
        "                if t_miss[i] > t_fcst:\n",
        "                    Pp = Plag[h][m].copy()\n",
        "                else:\n",
        "                    Pp = Plag[h][m].T.copy()\n",
        "                if i == 0:\n",
        "                    # Initialize projection onto updates\n",
        "                    P1 = np.matmul(Pp,C[[v_miss[i]]][:,:r].T)\n",
        "                else:\n",
        "                    # Projection on updates\n",
        "                    P1 = np.hstack([P1,np.matmul(Pp,C[[v_miss[i]]][:,:r].T)])\n",
        "\n",
        "            for i in range(t_miss.shape[0]):\n",
        "                # Standardize predicted and observed values\n",
        "                X_new_norm = (X_new[t_miss[i],v_miss[i]] - Mx[v_miss[i]])/Wx[v_miss[i]]\n",
        "                X_sm_norm  = (Res_old[\"X_sm\"][t_miss[i],v_miss[i]] - Mx[v_miss[i]])/Wx[v_miss[i]]\n",
        "\n",
        "                # Innovation: Gives [observed] data - [predicted data]\n",
        "                if i == 0:\n",
        "                    innov = X_new_norm - X_sm_norm\n",
        "                else:\n",
        "                    innov = np.hstack([innov,X_new_norm - X_sm_norm])\n",
        "            innov = innov.reshape((1,-1))\n",
        "\n",
        "            ins   = len(innov)\n",
        "            WW    = np.zeros((v_miss[-1]+1,v_miss[-1]+1))\n",
        "            WW[:] = np.nan\n",
        "\n",
        "            # Gives non-standardized series weights\n",
        "            for i in range(lag.shape[0]):\n",
        "                for j in range(lag.shape[0]):\n",
        "                    h = abs(lag[i] - lag[j])\n",
        "                    m = max(t_miss[i],t_miss[j])\n",
        "\n",
        "                    if t_miss[j] > t_miss[i]:\n",
        "                        Pp = Plag[h][m].copy()\n",
        "                    else:\n",
        "                        Pp = Plag[h][m].T.copy()\n",
        "\n",
        "                    if v_miss[i] == v_miss[j] and t_miss[i] != t_miss[j]:\n",
        "                        WW[v_miss[i],v_miss[j]] = 0\n",
        "                    else:\n",
        "                        WW[v_miss[i], v_miss[j]] = R[v_miss[i],v_miss[j]].copy()\n",
        "\n",
        "                    if j == 0:\n",
        "                        p2 = np.matmul(np.matmul(C[[v_miss[i]]][:,:r],Pp),C[[v_miss[j]]][:,:r].T) + WW[v_miss[i], v_miss[j]]\n",
        "                    else:\n",
        "                        p2 = np.hstack([p2,np.matmul(np.matmul(C[[v_miss[i]]][:,:r],Pp),C[[v_miss[j]]][:,:r].T) + WW[v_miss[i], v_miss[j]]])\n",
        "                if i == 0:\n",
        "                    P2 = p2.copy()\n",
        "                else:\n",
        "                    P2 = np.vstack([P2,p2])\n",
        "\n",
        "            try:\n",
        "                del temp\n",
        "            except NameError:\n",
        "                pass\n",
        "\n",
        "            # CHECK: can this be written better?\n",
        "            for i in range(v_news.shape[0]): # loop on v_news\n",
        "                # Convert to real units (unstadardized data)\n",
        "                if i == 0:\n",
        "                    totnews = np.matmul(np.matmul(np.matmul(np.matmul(Wx[[v_news[i]]],C[[v_news[i]]][:,:r]),P1),np.linalg.inv(P2)),innov.T)\n",
        "                    temp    = np.matmul(np.matmul(np.matmul(Wx[[v_news[i]]],C[[v_news[i]]][:,:r]),P1),np.linalg.inv(P2)) * innov\n",
        "                    gain    = np.matmul(np.matmul(np.matmul(Wx[[v_news[i]]],C[[v_news[i]]][:,:r]),P1),np.linalg.inv(P2))\n",
        "\n",
        "                    temp = temp.reshape((1,*temp.shape))\n",
        "                    gain = gain.reshape((1,*gain.shape))\n",
        "                else:\n",
        "                    temp_A = np.matmul(np.matmul(np.matmul(np.matmul(Wx[v_news[i]],C[[v_news[i]]][:,:r]),P1),np.linalg.inv(P2)),innov.T)\n",
        "                    temp_B = np.matmul(np.matmul(np.matmul(Wx[v_news[i]],C[[v_news[i]]][:,:r]),P1),np.linalg.inv(P2)) * innov\n",
        "                    temp_C = np.matmul(np.matmul(np.matmul(Wx[v_news[i]],C[[v_news[i]]][:,:r]),P1),np.linalg.inv(P2))\n",
        "\n",
        "                    totnews = np.hstack([totnews, temp_A])\n",
        "                    temp    = np.vstack([temp,temp_B[np.newaxis,]])\n",
        "                    gain    = np.vstack([gain, temp_C[np.newaxis,]])\n",
        "\n",
        "            # Initialize output objects\n",
        "            singlenews = np.zeros((v_news.shape[0],np.max(t_miss) - np.min(t_miss)+1,N))\n",
        "            actual     = np.zeros((N,1))\n",
        "            forecast   = np.zeros((N,1))\n",
        "            weight     = np.zeros((v_news.shape[0],N,1))\n",
        "            singlenews[:], actual[:], forecast[:], weight[:] = np.nan,np.nan,np.nan,np.nan\n",
        "\n",
        "            # Fill in output values\n",
        "            for i in range(innov.shape[1]):\n",
        "                actual[v_miss[i],0]   = X_new[t_miss[i],v_miss[i]].copy()\n",
        "                forecast[v_miss[i],0] = Res_old[\"X_sm\"][t_miss[i],v_miss[i]].copy()\n",
        "\n",
        "                for j in range(v_news.shape[0]):\n",
        "                    singlenews[j,t_miss[i]-min(t_miss),v_miss[i]] = temp[j,0,i].copy()\n",
        "                    weight[j,v_miss[i],:] = gain[j,:,i]/Wx[v_miss[i]]\n",
        "\n",
        "            singlenews = np.sum(singlenews, axis = 0) # Returns total news\n",
        "            v_miss     = np.sort(np.unique(v_miss))\n",
        "\n",
        "    # CHECK: weight seems suspicious\n",
        "    return y_old,y_new,singlenews,actual,forecast,weight[0],t_miss,v_miss,innov\n",
        "\n",
        "def para_const(X,P,lag):\n",
        "    # para_const()    Implements Kalman filter for \"News_DFM.m\"\n",
        "    #\n",
        "    #   Syntax:\n",
        "    #     Res = para_const(X,P,lag)\n",
        "    #\n",
        "    #   Description:\n",
        "    #     para_const() implements the Kalman filter for the news calculation\n",
        "    #     step. This procedure smooths and fills in missing data for a given\n",
        "    #     data matrix X. In contrast to runKF(), this function is used when\n",
        "    #     model parameters are already estimated.\n",
        "    #\n",
        "    #   Input parameters:\n",
        "    #     X: Data matrix.\n",
        "    #     P: Parameters from the dynamic factor model.\n",
        "    #     lag: Number of lags\n",
        "    #\n",
        "    #   Output parameters:\n",
        "    #     Res [struc]: A structure containing the following:\n",
        "    #       Res.Plag: Smoothed factor covariance for transition matrix\n",
        "    #       Res.P:    Smoothed factor covariance matrix\n",
        "    #       Res.X_sm: Smoothed data matrix\n",
        "    #       Res.F:    Smoothed factors\n",
        "    #\n",
        "    #\n",
        "    #\n",
        "    #  Kalman filter with specified paramaters\n",
        "    #  written for\n",
        "    #  \"MAXIMUM LIKELIHOOD ESTIMATION OF FACTOR MODELS ON DATA SETS WITH\n",
        "    #  ARBITRARY PATTERN OF MISSING DATA.\"\n",
        "    #  by Marta Banbura and Michele Modugno\n",
        "    #\n",
        "    #   Set model parameters and data preparation\n",
        "\n",
        "    # Set model parameters\n",
        "    Z_0 = P[\"Z_0\"].copy()\n",
        "    V_0 = P[\"V_0\"].copy()\n",
        "    A   = P[\"A\"].copy()\n",
        "    C   = P[\"C\"].copy()\n",
        "    Q   = P[\"Q\"].copy()\n",
        "    R   = P[\"R\"].copy()\n",
        "    Mx  = P[\"Mx\"].copy()\n",
        "    Wx  = P[\"Wx\"].copy()\n",
        "\n",
        "    # Prepare data\n",
        "    T = X.shape[0]\n",
        "\n",
        "    # Standardise x\n",
        "    Y = ((X - np.tile(Mx,(T,1)))/np.tile(Wx,(T,1))).T\n",
        "\n",
        "    # Apply Kalman filter and smoother\n",
        "    # See runKF() for details about FIS and SKF\n",
        "    Sf = SKF(Y,A,C,Q,R,Z_0,V_0) # Kalman filter\n",
        "    Ss = FIS(A,Sf)              # Smoothing step\n",
        "\n",
        "    # Calculate parameter output\n",
        "    Vs      = Ss[\"VmT\"][1:,:,:].copy() # Smoothed factor covariance for transition matrix\n",
        "    Vf      = Ss[\"VmU\"][1:,:,:].copy() # Filtered factor posterior covariance\n",
        "    Zsmooth = Ss[\"ZmT\"].copy()         # Smoothed factors\n",
        "    Vsmooth = Ss[\"VmT\"].copy()         # Smoothed covariance values\n",
        "\n",
        "    Plag = [Vs.copy()]\n",
        "\n",
        "\n",
        "    for jk in range(lag):\n",
        "        Plag.append(np.zeros(Vs.shape))\n",
        "        for jt in range(Plag[0].shape[0]-1,lag-1,-1):\n",
        "            As = np.matmul(np.matmul(Vf[jt-jk-1],A.T),\n",
        "                           np.linalg.pinv(np.matmul(np.matmul(A,Vf[jt-jk-1]),A.T) + Q))\n",
        "            Plag[jk+1][jt] = np.matmul(As, Plag[jk][jt])\n",
        "\n",
        "    # Prepare data for output\n",
        "    Zsmooth = Zsmooth.T\n",
        "\n",
        "    x_sm = np.matmul(Zsmooth[1:,:],C.T)                 # Factors to series representation\n",
        "    X_sm = np.tile(Wx,(T,1)) * x_sm + np.tile(Mx,(T,1)) # Standardized to unstandardized\n",
        "\n",
        "    # Loading dictionary with the results\n",
        "    Res = {\"Plag\" : Plag,\n",
        "           \"P\"    : Vsmooth,\n",
        "           \"X_sm\" : X_sm,\n",
        "           \"F\"    : Zsmooth[1:,:]\n",
        "    }\n",
        "\n",
        "    return Res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sfm82uFZ9cwA"
      },
      "source": [
        "#-------------------------------------------------Libraries\n",
        "import os\n",
        "from datetime import datetime as dt\n",
        "#from Functions.load_spec import load_spec\n",
        "#from Functions.load_data import load_data\n",
        "#from Functions.dfm import dfm\n",
        "import pickle\n",
        "#from Functions.summarize import summarize\n",
        "import pandas as pd\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#-------------------------------------------------Set dataframe to full view\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "\n",
        "#-------------------------------------------------User Inputs\n",
        "vintage      = '2016-06-29'                                                   # vintage dataset to use for estimation\n",
        "country      = 'US'                                                           # United States macroeconomic data\n",
        "sample_start = dt.strptime(\"2000-01-01\", '%Y-%m-%d').date().toordinal() + 366 # estimation sample\n",
        "\n",
        "\n",
        "#-------------------------------------------------Load model specification and dataset.\n",
        "# Load model specification structure `Spec`\n",
        "Spec = load_spec('/content/gdrive/MyDrive/Nowcasting-Python-master/Spec_US_example.xls')\n",
        "\n",
        "# Parse `Spec`\n",
        "SeriesID         = Spec.SeriesID\n",
        "SeriesName       = Spec.SeriesName\n",
        "Units            = Spec.Units\n",
        "UnitsTransformed = Spec.UnitsTransformed\n",
        "\n",
        "# Load data\n",
        "datafile   = os.path.join('/content/gdrive/MyDrive/Nowcasting-Python-master/data',country,vintage + '.xls')\n",
        "X,Time,Z   = load_data(datafile,Spec,sample_start)\n",
        "\n",
        "# Summarize dataset\n",
        "summarize(X,Time,Spec)\n",
        "\n",
        "\n",
        "#-------------------------------------------------Plot data\n",
        "# Raw vs transformed\n",
        "idxSeries = np.where(Spec.SeriesID == \"INDPRO\")[0][0]\n",
        "t_obs     = ~np.isnan(X[:,idxSeries])\n",
        "\n",
        "fig = make_subplots(rows=2, cols=1,\n",
        "                    subplot_titles=(\"Raw Observed Data\", \"Transformed Data\"))\n",
        "\n",
        "fig.append_trace(go.Scatter(\n",
        "    x=[dt.fromordinal(i - 366).strftime('%Y-%m-%d') for i in Time[t_obs]],\n",
        "    y=Z[t_obs,idxSeries],\n",
        "), row=1, col=1)\n",
        "\n",
        "fig.append_trace(go.Scatter(\n",
        "    x=[dt.fromordinal(i - 366).strftime('%Y-%m-%d') for i in Time[t_obs]],\n",
        "    y=X[t_obs,idxSeries],\n",
        "), row=2, col=1)\n",
        "\n",
        "\n",
        "fig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)'} ,\n",
        "                  title_text=\"Raw vs Transformed Data\",\n",
        "                  showlegend=False)\n",
        "fig.update_yaxes(title_text=Spec.Units[idxSeries], row=1, col=1)\n",
        "fig.update_yaxes(title_text=Spec.UnitsTransformed[idxSeries], row=2, col=1)\n",
        "fig.show()\n",
        "\n",
        "\n",
        "#-------------------------------------------------Run dynamic factor model (DFM) and save estimation output as 'ResDFM'.\n",
        "threshold = 1e-4 # Set to 1e-5 for more robust estimates\n",
        "Res = dfm(X,Spec,threshold)\n",
        "Res = {\"Res\": Res,\"Spec\":Spec}\n",
        "\n",
        "with open('ResDFM.pickle', 'wb') as handle:\n",
        "    pickle.dump(Res, handle)\n",
        "# TODO: Res and Spec should be separate, this will be fixed after the unit tests are created\n",
        "\n",
        "\n",
        "#-------------------------------------------------Plot Loglik across number of steps\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=np.arange(1,len(Res[\"Res\"][\"loglik\"][1:])+1),\n",
        "                         y=Res[\"Res\"][\"loglik\"][1:],\n",
        "                         mode='lines',\n",
        "                         name=\"LogLik\")\n",
        ")\n",
        "fig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)'} ,\n",
        "                  title_text=\"LogLik across number of steps taken\",\n",
        "                  showlegend=False\n",
        ")\n",
        "fig.update_yaxes(title_text=\"LogLik\")\n",
        "fig.update_xaxes(title_text=\"Number of steps\")\n",
        "fig.show()\n",
        "\n",
        "\n",
        "#-------------------------------------------------Plot common factor and standardized data.\n",
        "# select INDPRO data series\n",
        "idxSeries = np.where(Spec.SeriesID == \"INDPRO\")[0][0]\n",
        "\n",
        "# Create traces\n",
        "fig = go.Figure()\n",
        "for i in range(Res[\"Res\"][\"x_sm\"].shape[1]):\n",
        "    fig.add_trace(go.Scatter(x=[dt.fromordinal(i - 366).strftime('%Y-%m-%d') for i in Time],\n",
        "                             y=Res[\"Res\"][\"x_sm\"][:,i],\n",
        "                             mode='lines',\n",
        "                             name=Spec.SeriesID[i],\n",
        "                             line={'width':.9})\n",
        ")\n",
        "fig.add_trace(go.Scatter(x=[dt.fromordinal(i - 366).strftime('%Y-%m-%d') for i in Time],\n",
        "                         y=Res[\"Res\"][\"Z\"][:,0]*Res[\"Res\"][\"C\"][idxSeries,0],\n",
        "                         mode='lines',\n",
        "                         name=\"Common Factor\",\n",
        "                         line=dict(color='black', width=1.5))\n",
        ")\n",
        "\n",
        "# Plot common factor and standardized data\n",
        "fig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)'} ,\n",
        "                  title_text=\"Common Factor and Standardized Data\"\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "\n",
        "#-------------------------------------------------Plot projection of common factor onto Payroll Employment and GDP\n",
        "# Two plots in one graph\n",
        "fig = make_subplots(rows=2, cols=1,\n",
        "                    subplot_titles=(\"Payroll Employment\", \"Real Gross Domestic Product\"))\n",
        "\n",
        "# Create an array of the data series that we are interested in looping through to plot the projection\n",
        "series = [\"PAYEMS\",\"GDPC1\"]\n",
        "\n",
        "# For a particular series:\n",
        "#       1.) plot the common factor\n",
        "#       2.) plot the data series (with NAs removed)\n",
        "for i in range(len(series)):\n",
        "\n",
        "    idxSeries    = np.where(Spec.SeriesID == series[i])[0][0]\n",
        "    t_obs        = ~np.isnan(X[:,idxSeries])\n",
        "\n",
        "    CommonFactor = np.matmul(Res[\"Res\"][\"C\"][idxSeries,:5].reshape(1,-1),Res[\"Res\"][\"Z\"][:,:5].T) * \\\n",
        "                   Res[\"Res\"][\"Wx\"][idxSeries] + Res[\"Res\"][\"Mx\"][idxSeries]\n",
        "\n",
        "    fig.append_trace(go.Scatter(\n",
        "        x=[dt.fromordinal(i - 366).strftime('%Y-%m-%d') for i in Time],\n",
        "        y=CommonFactor[0,:],\n",
        "        name=\"Common Factor ({})\".format(series[i])\n",
        "    ), row=i+1, col=1)\n",
        "\n",
        "    fig.append_trace(go.Scatter(\n",
        "        x=[dt.fromordinal(i - 366).strftime('%Y-%m-%d') for i in Time[t_obs]],\n",
        "        y=X[t_obs,idxSeries],\n",
        "        name=\"Data ({})\".format(series[i])\n",
        "    ), row=i+1, col=1)\n",
        "\n",
        "    fig.update_yaxes(title_text=Spec.Units[idxSeries] + \" ({})\".format(Spec.UnitsTransformed[idxSeries]), row=i+1, col=1)\n",
        "\n",
        "fig.update_layout({'plot_bgcolor': 'rgba(0, 0, 0, 0)'} ,\n",
        "                  title_text=\"Projection of Common Factor\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6jmYnwRV4Ey"
      },
      "source": [
        "#-------------------------------------------------Libraries\n",
        "#from Functions.load_data import load_data\n",
        "#from Functions.load_spec import load_spec\n",
        "#from Functions.update_Nowcast import update_nowcast\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "\n",
        "#-------------------------------------------------User Inputs\n",
        "series = 'GDPC1'  # Nowcasting real GDP (GDPC1) <fred.stlouisfed.org/series/GDPC1>\n",
        "period = '2016q4' # Forecasting target quarter\n",
        "\n",
        "\n",
        "#-------------------------------------------------Load model specification and first vintage of data.\n",
        "Spec   = load_spec('/content/gdrive/MyDrive/Nowcasting-Python-master/Spec_US_example.xls')\n",
        "\n",
        "\n",
        "#-------------------------------------------------Load DFM estimation results structure `Res`\n",
        "with open('ResDFM.pickle', 'rb') as handle:\n",
        "    Res = pickle.load(handle)\n",
        "\n",
        "\n",
        "#-------------------------------------------------Update nowcast and decompose nowcast changes into news.\n",
        "# Nowcast update from week of December 7 to week of December 16, 2016\n",
        "vintage_old  = '2016-12-16'\n",
        "vintage_new  = '2016-12-23'\n",
        "datafile_old = os.path.join('/content/gdrive/MyDrive/Nowcasting-Python-master/data','US',vintage_old + '.xls')\n",
        "datafile_new = os.path.join('/content/gdrive/MyDrive/Nowcasting-Python-master/data','US',vintage_new + '.xls')\n",
        "\n",
        "# Load datasets for each vintage\n",
        "X_old,_,_    = load_data(datafile_old,Spec)\n",
        "X_new,Time,_ = load_data(datafile_new,Spec)\n",
        "\n",
        "Res = Res['Res']\n",
        "\n",
        "update_nowcast(X_old,X_new,Time,Spec,Res,series,period,vintage_old,vintage_new)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}